{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# K-Means-Clustering"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Un\u00fcberwachtes Lernen: Clustering\n", "    \n", "Diese Woche liegt der Fokus auf dem Gebiet des un\u00fcberwachten Lernens. Wir werden mit Methoden des un\u00fcberwachten Lernens (engl. unsupervised learning) wie z. B. Clustering arbeiten. Sie werden (optional) lernen, warum und wie wir die Dimensionalit\u00e4t der Originaldaten reduzieren k\u00f6nnen und was die wichtigsten Ans\u00e4tze zur Gruppierung \u00e4hnlicher Datenpunkte sind.  \n", "\n", "\n", "### Einf\u00fchrung\n", "\n", "Bislang haben wir Methoden des maschinellen Lernens weitgehend im Kontext des _\u00fcberwachten_ Lernens vorgestellt.  Das hei\u00dft, wir haben \u00fcber Datens\u00e4tze f\u00fcr maschinelles Lernen gesprochen, die Eingaben $x$ und Ausgaben $y$ haben. Dabei ist das Ziel eines Algorithmus f\u00fcr maschinelles Lernen, zu lernen, wie man $y$ aus $x$ vorhersagen kann. \u00dcberwachtes Lernen dominiert traditionell die Mehrheit der praktischen Data-Science-Anwendungen. Aber in dieser Aufgabe werden wir Ans\u00e4tze diskutieren, die von diesen abweichen. Insbesondere werden wir die Funktion des un\u00fcberwachten Lernens betrachten, bei dem uns _nicht_ entsprechende Eingabe/Ausgabe-Paare gegeben sind, sondern bei dem uns _nur_ die Eingaben $x$ gegeben sind. Im Vergleich zu Klassifizierungs- und Regressionsmethoden besteht der Grundsatz darin, dass die Eingabedaten unbeschriftet sind (d. h. es sind keine Labels oder Klassen gegeben). Das wirft nat\u00fcrlich die offensichtliche Frage auf: Wenn uns keine Zielausgabe gegeben wird, was genau sollen wir dann vorhersagen? Und tats\u00e4chlich ist die Aufgabe des un\u00fcberwachten Lernens auf einer gewissen Ebene mehrdeutig. Die allgemeine Philosophie des un\u00fcberwachten Lernens ist jedoch, dass wir eine Art von Struktur in den Daten entdecken wollen und dass der Algorithmus die Struktur der Daten ohne jegliche Hilfe erlernt. Verschiedene Methoden des un\u00fcberwachten Lernens arbeiten auf sehr unterschiedliche Weise und entdecken sehr unterschiedliche Arten von Strukturen, aber sie haben alle dieses \u00e4hnliche Element. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.1:</b> Was sind die beiden Hauptunterschiede hinsichtlich Datenerstellung und Training/Auswertung zwischen \u00fcberwachtem und un\u00fcberwachtem Lernen?\n", "Tipp: Denken Sie an die Eingabe und Ausgabe der beiden Kategorien!\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clustering\n", "\n", "In der vorherigen Teilaufgabe haben wir eine Kategorie von Modellen f\u00fcr un\u00fcberwachtes maschinelles Lernen untersucht: Die Dimensionalit\u00e4tsreduktion. Nun werden wir zu einer anderen Klasse von un\u00fcberwachten maschinellen Lernmodellen \u00fcbergehen: Clustering-Algorithmen. Die Hauptidee hinter Clustering ist ziemlich einfach. Clustering-Algorithmen versuchen, aus den Eigenschaften der Daten eine optimale Einteilung oder diskrete Kennzeichnung von Punktgruppen zu lernen. Im Grunde genommen sagen wir uns: \"Ich habe hier diese Punkte, und ich denke, dass sie sich in Gruppen organisieren. Es w\u00e4re sch\u00f6n, diese Dinge konkreter zu beschreiben und, wenn ein neuer Punkt hinzukommt, ihn der richtigen Gruppe zuzuordnen.\" Diese allgemeine Idee ermutigt zur Exploration und er\u00f6ffnet eine Vielzahl von Algorithmen f\u00fcr das Clustering.\n", "<br>\n", "<br>\n", "<figure><img align=\"center\" src=\"https://habrastorage.org/getpro/habr/post_images/8b9/ae5/586/8b9ae55861f22a2809e8b3a00ef815ad.png\"><figcaption>*Beispiele f\u00fcr Ergebnisse von verschiedenen Algorithmen aus scikit-learn*</figcaption></figure>\n", "\n", "Die oben aufgef\u00fchrten Algorithmen decken nicht alle Clustering-Methoden ab, aber sie sind die am h\u00e4ufigsten verwendeten. Heute werden wir n\u00e4her auf das K-Means-Clustering eingehen, welches der beliebteste Clustering-Algorithmus ist.\n", "\n", "### K-means clustering\n", "\n", "\n", "Als erstes Beispiel f\u00fcr einen un\u00fcberwachten Lernalgorithmus im Bereich des Clustering betrachten wir den K-Means-Clustering-Algorithmus. Der k-means-Algorithmus ist der beliebteste und gleichzeitig einfachste aller Clustering-Algorithmen. Bevor wir jedoch die formale Definition und Funktion vorstellen, betrachten wir das k-means Clustering aus einem eher visuellen/geometrischen Blickwinkel. Einfach als eine M\u00f6glichkeit, Datenpunkte zu clustern. Das Ziel des k-means-Algorithmus ist es, eine Menge von $k$ \"Zentren\" innerhalb eines unbeschrifteten mehrdimensionalen Datensatzes zu finden (d. h. Punkte $\\mu^{(i)} \\in \\mathbb{R}^n$), wobei $k$ eine vorher festgelegte Zahl ist. Dies wird durch eine einfache Vorstellung davon erreicht, wie das optimale Clustering aussieht:\n", "\n", "- Das \"Clusterzentrum\" ist das arithmetische Mittel aller zum Cluster geh\u00f6renden Punkte.\n", "- Jeder Punkt ist n\u00e4her an seinem eigenen Clusterzentrum als an anderen Clusterzentren.\n", "\n", "Diese beiden Annahmen sind die Grundlage des **k**-Mittelwertmodells. Auf diese Weise k\u00f6nnen wir auch jeden Punkt mit seinem n\u00e4chstgelegenen Zentrum assoziieren und dies als Hinweis darauf verwenden, zu welchem Cluster er geh\u00f6rt. Mit einem *k*-Means-Modell sind wir also auch in der Lage, neue Datenpunkte zu klassifizieren.\n", "\n", "Und so funktioniert es:\n", "\n", "**Algorithmus**: K-means\n", "\n", "**Gegeben:** Datensatz $x^{(i)}, i=1,\\ldots,m$\n", "\n", "**Initialisieren Sie:**\n", "1. W\u00e4hlen Sie die Anzahl von Clustern $k$, die Sie f\u00fcr optimal halten.  \n", "   \n", "2. Initialisieren Sie $k$ Punkte als \"Zentroide\" zuf\u00e4llig im Raum unserer Daten.  \n", "    $\\mu^{(j)} := \\mbox{RandomChoice}(x^{(1:m)}), \\; j=1,\\ldots,k$  \n", "\n", "**Wiederholen Sie den Vorgang bis zur Konvergenz:**\n", "3. Ordnen Sie jede Beobachtung ihrem n\u00e4chstgelegenen Schwerpunkt zu.  \n", "    $y^{(i)} := argmin_j \\|\\mu^{(j)} - x^{(i)}\\|_2^2, \\; i=1,\\ldots,m$\n", "4. Aktualisieren Sie die Zentroide auf den Mittelpunkt aller zugewiesenen Beobachtungss\u00e4tze.   \n", "    $\\displaystyle \\mu^{(j)} := \\sum_{i=1}^m \\frac{x^{(i)} \\mathrm{1}\\{y^{(i)} = j\\}}{\\mathrm{1}\\{y^{(i)} = j\\}}, \\;\\; j=1,\\ldots,k$\n", "5. Wiederholen Sie die Schritte 3 und 4 bis alle Zentroide stabil sind (d. h. sich in Schritt 4 nichts mehr \u00e4ndert).\n", "\n", "Was ist die _Verlustfunktion_, die wir hier optimieren? Es ist die Summe der Abst\u00e4nde jedes Punktes zu seinem n\u00e4chstgelegenen Schwerpunkt. Obwohl wir dies hier nicht formal beweisen werden, steht fest, dass der Algorithmus bei jedem Schritt die Verlustfunktion _verringert_ (die Intuition ist, dass jeder Schritt, sowohl die Neuzuordnung von Clustern als auch die Berechnung neuer Zentren, nur den Gesamtverlust verringern kann, und daher wird der Gesamtverlust nur auf diese Weise verringert).  Da es au\u00dferdem nur eine endliche Anzahl m\u00f6glicher Clusterungen gibt (nat\u00fcrlich exponentiell gro\u00df, aber immer noch endlich), wissen wir, dass der Algorithmus nach einer endlichen Anzahl von Schritten konvergiert (d. h. die Clusterzuweisungen bleiben von einer Iteration zur n\u00e4chsten konstant, was bedeutet, dass sich die Zentren ebenfalls nicht \u00e4ndern und der Algorithmus einen Fixpunkt erreicht hat).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In der folgenden Abbildung k\u00f6nnen Sie sehen, was in jeder Iteration des k-Means-Algorithmus passieren sollte. \n", "<figure><img align=\"center\" src=\"images\\05.11-expectation-maximization.png\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.2:</b> Beschreiben Sie in Ihren eigenen Worten, was bei der Verwendung von k-Means passiert.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Entwickeln einer einfachen k-Means-Implementierung\n", "Zun\u00e4chst einmal m\u00fcssen wir alle ben\u00f6tigten Bibliotheken und unsere Daten laden. Wir beginnen mit den Standard-Importen: numpy, pyplot, seaborn und scikitlearn-datasets."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# Import numpy, pyplot, datasets\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn import datasets\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "from IPython import display\n", "import ipywidgets as widgets\n", "plt.style.use(['seaborn-darkgrid'])\n", "plt.rcParams['figure.figsize'] = (12, 9)\n", "plt.rcParams['font.family'] = 'DejaVu Sans'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Als n\u00e4chsten Schritt wollen wir einen zweidimensionalen Datensatz erzeugen, der vier verschiedene Blobs enth\u00e4lt. Wir werden den folgenden Satz von 300 Datenpunkten verwenden. Um zu verdeutlichen, dass es sich um einen un\u00fcberwachten Algorithmus handelt, lassen wir die Beschriftungen in der Visualisierung weg."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_blobs\n", "RANDOM_STATE = 42\n", "X_blob, y_blob = make_blobs(\n", "    n_samples=300, centers=4,cluster_std=0.60, random_state=RANDOM_STATE, shuffle=True\n", ")\n", "plt.scatter(X_blob[:, 0],X_blob[:, 1], s=50);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.4.3:</b> F\u00fcllen Sie den folgenden Code aus, um eine einfache Implementierung von k-Means selbst zu schreiben!\n", "<ul>\n", "<li> Initialisieren Sie k Punkte als \"Zentroide\" zuf\u00e4llig mit RandomState\n", "<li> Dann wiederholen Sie die Schrittzuweisung, aktualisieren die Zentroide und pr\u00fcfen die Konvergenz\n", "\n", "</li>\n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# use the following import to simplify the implementation\n", "from sklearn.metrics import pairwise_distances_argmin\n", "\n", "# 1. Select the number of clusters: This has to be done before the function is called\n", "# rseed is the seed for generation of random numbers\n", "def find_clusters(X: np.ndarray, n_clusters: int, rseed: int):\n", "    \n", "    # 2. Initialize k points as \"centroids\" randomly \n", "    # Please initialize the random number generator with specified random state\n", "    # STUDENT CODE HERE\n", "\n", "    # STUDENT CODE until HERE    \n", "    # return the cluster centers and the labels\n", "    return centers, labels\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Analysieren von Blob-Daten"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.4.4:</b> Verwenden Sie Ihre Funktion, um ein Clustering der oben erzeugten Daten zu berechnen. Der zuf\u00e4llige Seed ist bereits festgelegt. Visualisieren Sie die gefundenen Clusterbeschriftungen mit verschiedenen Farben. Da wir die Daten selbst generiert haben, wissen wir, wie viele Cluster wir finden wollen. Welchen Wert m\u00fcssen wir also f\u00fcr k angeben?\n", "\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["RANDOM_STATE = 42\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.5:</b> Experimentieren Sie mit dem Zufallsgenerator. Was beobachten Sie? Was k\u00f6nnten wir tun, um dies zu \u00fcberwinden?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Analysieren eines weiteren synthetischen Datensatzes\n", "Wir haben unseren ersten Datensatz mit der Funktion *make_blobs* erzeugt. Um ein besseres Gef\u00fchl f\u00fcr die Eigenschaften von k-Means zu bekommen, wollen wir die Ergebnisse mit einem weiteren Datensatz analysieren. Wir importieren die Funktion *make_moons* aus sklearn datasets und erzeugen einen weiteren Datensatz, bestehend aus 300 Datenpunkte. Der Rauschwert $0.05$, um eine gewisse Abweichung in unserem Datensatz einzubeziehen.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.4.6:</b> Verwenden Sie Ihre Funktion zum Clustern der Daten und suchen Sie nach zwei Clustern. Zeigen Sie das Ergebnis in einem Streudiagramm.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_moons\n", "X_moon, y_moon = make_moons(300, noise=.05, random_state=RANDOM_STATE)\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.4.7:</b> Was f\u00e4llt Ihnen auf? K\u00f6nnen Sie erkl\u00e4ren, warum dies geschieht? Welche Bedingung des k-Means-Algorithmus f\u00fchrt zu diesem Ergebnis?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.8:</b> Was passiert, wenn wir den Wert von k auf vier statt auf zwei setzen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ob das Ergebnis aussagekr\u00e4ftig ist, ist eine Frage, die schwer zu beantworten ist. In den folgenden Aufgaben werden wir verschiedene Ans\u00e4tze kennenlernen."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Evaluieren eines Clusterings\n", "\n", "Wie bereits zu Beginn der heutigen Session erw\u00e4hnt, ist die Auswertung von Clustering-Ergebnissen schwierig. Bei den synthetischen Datens\u00e4tzen, die wir oben erzeugt haben, haben wir gelabelte Daten, da die Blobs / Monde als vorgegebene Klassen (Cluster) gesehen werden k\u00f6nnen. Wir k\u00f6nnen die \u00dcbereinstimmung unserer Clustering-Ergebnisse mit dem gegebenen Label \u00fcberpr\u00fcfen. Aber im Allgemeinen verwenden wir un\u00fcberwachte Methoden, wenn wir keine Labels f\u00fcr unsere Daten haben. Was k\u00f6nnen wir also tun, wenn es kein Label zum \u00dcberpr\u00fcfen gibt? Wir sind nur dann in der Lage, unser Clustering zu evaluieren, wenn wir ein Ma\u00df f\u00fcr die Qualit\u00e4t eines Clusterns definieren k\u00f6nnen. Die Auswertung eines Clustering gibt Aufschluss dar\u00fcber, nach wie vielen Clustern gesucht werden soll. Es gibt *interne* und *externe* G\u00fctemetriken. Externe Metriken verwenden die Informationen \u00fcber die bekannte wahre Aufteilung (wir haben Labels), w\u00e4hrend interne Metriken keine externen Informationen verwenden und die G\u00fcte von Clustern nur auf Basis der Ausgangsdaten bewerten. Die optimale Anzahl von Clustern wird normalerweise in Bezug auf einige interne Metriken definiert. \n", "\n", "#### Verwenden von gegebenen Labels in einer einfachen Tabelle\n", "Zun\u00e4chst verwenden wir die gegebenen Informationen der Labels. Zu diesem Zweck k\u00f6nnen wir die __cross-tabulation__ verwenden, die eine Methode zur quantitativen Analyse der Beziehung zwischen mehreren Variablen bereitstellt. In unserem Fall sind die beiden Variablen unsere Labels und die gesch\u00e4tzten Cluster."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.4.9:</b> \n", "<ul>\n", "<li> Abrufen der Listen der Clustering-Ergebnisse f\u00fcr den Blob-Datensatz und die Label Daten\n", "<li> Verwenden Sie die Bibliothek *pandas*, um die cross-tabulation durchzuf\u00fchren.\n", "<li> Setzen Sie den Index und die Spalten der Tabelle richtig. Der Index sollte den echten / realen Cluster anzeigen, und die Spalte sollte beschreiben, welchem Cluster die Punkte zugeordnet wurden.\n", "</li>\n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Fragen 3.4.10:</b> Entsprechen die Cluster den realen Klassen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Messung der Clustering-Qualit\u00e4t ohne Etiketten: Silhouetten-Koeffizient\n", "\n", "Im Gegensatz zu der oben beschriebenen Metrik impliziert der Silhouettenkoeffizient nicht das Wissen \u00fcber die wahren Labels der Objekte. Er erm\u00f6glicht es uns, die Qualit\u00e4t des Clustering nur anhand der anf\u00e4nglichen, nicht gelabelten Datenpunkte und des Clustering-Ergebnisses zu sch\u00e4tzen. Zu Beginn wird f\u00fcr jede Beobachtung der Silhouettenkoeffizient berechnet. Sei $a$ der Mittelwert des Abstands eines Objekts zu anderen Objekten innerhalb eines Clusters und $b$ der mittlere Abstand eines Objekts zu Objekten des n\u00e4chstgelegenen Clusters (der sich von dem unterscheidet, zu dem das Objekt geh\u00f6rt). Dann sei das Silhouettenma\u00df f\u00fcr dieses Objekt $$s = \\frac{b - a}{\\max(a, b)}.$$\n", "\n", "Die Silhouette eines Datenpunktes ist ein Mittelwert der Silhouettenwerte aus diesem Datenpunkt. Der Silhouettenabstand zeigt also an, wie stark sich der Abstand zwischen den Objekten der gleichen Klasse vom mittleren Abstand zwischen den Objekten aus verschiedenen Clustern unterscheidet. Dieser Koeffizient nimmt Werte im Bereich $[-1, 1]$ an. Werte nahe bei -1 entsprechen schlechten Clustering-Ergebnissen, w\u00e4hrend Werte n\u00e4her bei 1 dichten, gut definierten Clustern entsprechen. Je h\u00f6her also der Silhouettenwert ist, desto besser sind die Ergebnisse des Clusterns."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.11:</b> Berechnen Sie den Silhouettenkoeffizienten und erhalten Sie das Ergebnis f\u00fcr die oben ausgew\u00e4hlten Blobdaten mit dem entsprechenden Wert f\u00fcr k. Welche Schlussfolgerung k\u00f6nnen wir hier ziehen? Tipp: Verwenden Sie die scikit-learn-Bibliothek f\u00fcr die Berechnung.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Clustering mit scikit-learn Implementation\n", "\n", "Nachdem wir unsere eigene Implementierung von K-means analysiert haben, k\u00f6nnen wir nun die von scikit-learn vorgegebene Implementierung verwenden. Das eingebaute K-Means verwendet standardm\u00e4\u00dfig die Initialisierung von KMeans++. Weitere Details zu dieser Initialisierungsprozedur finden Sie im Anhang. Um konsistente und reproduzierbare Ergebnisse zu erhalten, m\u00fcssen wir nur den Zufallszustand angeben, wie wir es in unserer eigenen Version getan haben. Ein Vorteil von scikit-learn ist, dass wir mit einer eingebauten Funktion direkt Cluster-Labels f\u00fcr neue Datenpunkte sch\u00e4tzen k\u00f6nnen. Dabei werden die neuen Datenpunkten den vorhandenen Clustern zugeordnet. Da sich k-means den Mittelwert jedes Clusters (die \"Zentroide\") merkt, ist es m\u00f6glich, zu jeder neuen Probe den n\u00e4chstgelegenen Zentroid zu finden und das entsprechende Label zuzuweisen.\n", "\n", "Nun beginnen wir, die Ergebnisse von oben mit scikit-learn zu reproduzieren. Wir werden auswerten, ob wir im Blob-Datensatz die gleichen Cluster wie zuvor finden.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.4.12:</b> \n", "<ul>\n", "<li> Importieren Sie KMeans aus sklearn.cluster.\n", "<li> Erstellen Sie mit <code>KMeans()</code> eine KMeans-Instanz, um die richtige Anzahl von Clustern zu finden. Um die Anzahl der Cluster anzugeben, verwenden Sie das Schl\u00fcsselwort-Argument n_clusters.\n", "<li> Verwenden Sie die Methode <code>.fit()</code> von model, um das Modell an das Array von Punkten anzupassen.\n", "<li> Verwenden Sie die Methode <code>.predict()</code> des Modells, um die Cluster-Labels unseres Datensatzes vorherzusagen, und weisen Sie das Ergebnis den Labels zu\n", "</li>\n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "#plotting is given\n", "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=labels_blob,\n", "            s=50, cmap='viridis');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.13:</b> Was beobachten Sie? Welchen Unterschied k\u00f6nnen Sie erkennen, wenn Sie die Ergebnisse unserer eigenen Implementierung und der scikit-learn-L\u00f6sung vergleichen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### W\u00e4hlen Sie einen geeigneten Wert f\u00fcr $k$\n", "Eine offensichtliche Frage, die sich bei der Verwendung von K-Means stellt, ist: Wie w\u00e4hlt man die Hyperparameter f\u00fcr den Algorithmus, wie z. B. die Anzahl der Zentren $k$? Im Allgemeinen haben wir nur die Datenpunkte und den Cluster, dem sie zugeordnet sind. Nun brauchen wir ebenso Metriken, die von den Datenpunkten abh\u00e4ngen, um den perfekten Wert f\u00fcr $k$ auszuw\u00e4hlen.\n", "\n", "\n", "\n", "#### Verwendung des Silhouette-Koeffizienten\n", "Eine M\u00f6glichkeit ist die Verwendung des Silhouettenkoeffizienten, den wir oben eingef\u00fchrt haben. Mit Hilfe der Silhouette k\u00f6nnen wir die optimale Anzahl von Clustern $k$ identifizieren (wenn wir sie nicht schon aus den Daten kennen), indem wir die Anzahl von Clustern nehmen, die den Silhouettenkoeffizienten maximiert. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.4.14:</b> Berechnen Sie den Silhouettenkoeffizienten f\u00fcr verschiedene Werte von k und w\u00e4hlen Sie die optimale Zahl! Der Code zum Plotten verschiedener ks (eine Reihe von k-Hyperparametern) versus dem Score wurde f\u00fcr Sie bereits geschrieben, f\u00fcllen Sie also nur noch den Rest des Codes aus.\n", "\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["def calculate_k_silhouette(X:np.ndarray, k_min:int, k_max:int):\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "    return ks, scores"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["#set random state\n", "RANDOM_STATE = 42\n", "# call your function with a suitable range for k\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "# Plot ks vs silhouette coefficient\n", "plt.plot(ks, silhouette_scores, '-o')\n", "plt.xlabel('number of clusters, k')\n", "plt.ylabel('Silhouette Score')\n", "plt.xticks(ks)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.15:</b> Welche Anzahl von Clustern sollten wir entsprechend dem Silhouetten-Score w\u00e4hlen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Verwendung der Ellenbogenmethode\n", "Ein gutes Clustering hat enge Cluster und die Datenpunkte in jedem Cluster sind zusammengeballt. Daher k\u00f6nnen wir die Streuung jedes Clusters bewerten, wobei eine geringere Streuung besser ist. K-means optimiert von Natur aus die Summe der quadratischen Abst\u00e4nde zwischen den Beobachtungen und ihren Zentren. Dies ist die *Verlustfunktion* f\u00fcr den k-Means-Algorithmus. \n", "\n", "$$ J(C) = \\sum_{k=1}^K\\sum_{i~\\in~C_k} ||x_i - \\mu_k|| \\rightarrow \\min\\limits_C,$$\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Wir k\u00f6nnen die Verlustfunktion als Qualit\u00e4tsma\u00df verwenden. Das scheint vern\u00fcnftig zu sein -- wir wollen, dass unsere Beobachtungen so nah wie m\u00f6glich an ihren Zentren liegen. Bewerten Sie nun, was das Qualit\u00e4tsma\u00df \u00fcber die Verlustfunktion f\u00fcr verschiedene Werte von $k$ anzeigt!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.4.16:</b> Der Code zum Plotten verschiedener ks gegen den Verlust wurde bereits f\u00fcr Sie geschrieben, f\u00fcllen Sie also den Rest des Codes aus. Die wichtigsten Schritte sind:\n", "<ul>\n", "<li> Definieren Sie die Liste der k-Werte\n", "<li> F\u00fchren Sie f\u00fcr jeden der Werte von k die folgenden Schritte aus:\n", "    <ul>\n", "        <li>1. Anpassen eines K-Means-Modells an die gegebenen Datenproben mit k Clustern \n", "        <li>2. Berechnen Sie den Verlust\n", "</li>\n", "</ul>\n", "</li>\n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["def calculate_k_loss(X:np.ndarray, k_min:int, k_max:int):\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "    return ks, losses"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["#set random state\n", "RANDOM_STATE = 42\n", "# call your function with a suitable range for k\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "# Plot ks vs inertias\n", "plt.plot(ks, losses, '-o')\n", "plt.xlabel('number of clusters, k')\n", "plt.ylabel('$ J(C)$')\n", "plt.xticks(ks)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.17:</b> Welches Problem haben wir, wenn wir nur versuchen, den Verlust $J(C_k)$ zu minimieren?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", "", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.4.18:</b> Was k\u00f6nnen wir tun, um dieses Problem zu vermeiden? Wie viele Cluster sollten wir dann in unserem Fall w\u00e4hlen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", "", "", "", "", "", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Zusammenfassend l\u00e4sst sich sagen, dass die Auswahl der Anzahl von Clustern typischerweise eine wage Kunst ist und es _sehr_ schwierig ist, aus der Ausf\u00fchrung von k-means etwas \u00fcber die \"wirkliche\" Anzahl von Clustern in den Daten abzuleiten (tats\u00e4chlich sollten Sie dies niemals versuchen)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Mean-Shift Clustering\n", "\n", "Mean-Shift Clustering ist ein Schwerpunkt basierter Algorithmus. Hierbei wird der Schwerpunkt \u00fcber das relative Auftreten der Datenpunkte definiert. Dies bedeutet in der Praxis, das kreisf\u00f6rige Strukturen gesucht werden, die so im Bild liegen, dass m\u00f6glichst viele Datenpunkte in der Struktur liegen. Kritisch hierbei ist die Gr\u00f6\u00dfe der Struktur, die auch als _Bandwith_ bezeichnet wird. In der Praxis werden ganz viele solcher Strukturen, die zuf\u00e4llig im Bild verteilt werden, berechnet und bei \u00dcberlappung verschmolzen. Dies hat den Vorteil, dass im Gegensatz zu k-Means die Anzahl an Clustern nicht vorgegeben werden muss. Daf\u00fcr ist der Algorithmus langsamer.   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gegeben sind nun die bereits bekannten Blob-Daten:"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_blobs\n", "import matplotlib.pyplot as plt\n", "RANDOM_STATE = 42\n", "X_blob, y_blob = make_blobs(\n", "    n_samples=300, centers=4,cluster_std=0.60, random_state=RANDOM_STATE, shuffle=True\n", ")\n", "plt.figure(figsize=(15,10))\n", "plt.scatter(X_blob[:, 0],X_blob[:, 1], s=50)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Aufgabe 3.4.19:</b> Verwenden Sie <code>sklearn</code> um die Daten mittels Mean-Shift Clustering zu Clustern und speichern Sie das Ergebnis in einer Variablen mit dem Namen <code>predicted_labels</code>.\n", "    \n", "    \n", "_Hinweis: Verweden Sie <code>sklearn.cluster.MeanShift</code> und eine <code>bandwidth</code> von 4._\n", "</div>"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "plt.figure(figsize=(15,10))\n", "plt.scatter(\n", "    X_blob[:, 0], X_blob[:, 1], c=predicted_labels\n", ")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Frage 3.4.20:</b> Was passiert, wenn Sie die <code>bandwith</code> auf 0.4 bzw. 40 statt 4 setzten\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### DBSCAN\n", "\n", "Auch DBSCAN arbeitet dichtebasiert. Diesem Verfahren liegt die Annahme zugrunde, dass Cluster Orte hoher Dichte sind, die durch Bereiche niedriger Dichte voneinander getrennt werden. Hierzu werden sogenannte _Core Samples_ bestimmt. Hierbei handelt es sich um Datenpunkte, die in einem sehr dichten Bereich liegen. Somit wird die Position eines Clusters von seinen Core Samples bestimmt. Daraus folgt ebenso, dass die Core Samples eines Clusters nah beieinander liegen. Anschlie\u00dfend werden die verbleibenden nicht - Core Samples dem naheliegensten Cluster zugewiesen, sofern diese nicht zu weit entfernt von den Core Samples liegen. Somit kann es Datenpunkte geben, die keinem Cluster zugeordnet werden.\n", "DBSCAN besitzt i.d.R. zwei Parameter, n\u00e4mlich:\n", "\n", "1.) Die Mindestanzahl an Datenpunkte f\u00fcr einen Cluster sowie\n", "\n", "2.) Epsilon, welches welches den maximalen Abstand zwischen zwei Datenpunkten, die im gleichen Cluster liegen, angibt.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gegeben seien wieder die bekannten Blob-Daten:"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_blobs\n", "import matplotlib.pyplot as plt\n", "RANDOM_STATE = 42\n", "X_blob, y_blob = make_blobs(\n", "    n_samples=300, centers=4,cluster_std=0.60, random_state=RANDOM_STATE, shuffle=True\n", ")\n", "plt.figure(figsize=(15,10))\n", "plt.scatter(X_blob[:, 0],X_blob[:, 1], s=50)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Aufgabe 3.4.21:</b> Verwenden Sie <code>sklearn</code> um die Daten mittels DBSCAN zu Clustern und speichern Sie das Ergebnis in einer Variablen mit dem Namen <code>predicted_labels</code>.\n", "    \n", "    \n", "_Hinweis: Verweden Sie <code>sklearn.cluster.DBSCAN</code>, mit <code>eps=0.5</code> und <code>min_samples=5</code>_\n", "</div>"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "plt.figure(figsize=(15,10))\n", "plt.scatter(\n", "    X_blob[:, 0], X_blob[:, 1], c=predicted_labels\n", ")\n", "plt.show()\n", "print('Number of classes: {}'.format(len(list(filter(lambda x: x != -1 , set(predicted_labels))))))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Frage 3.4.22:</b> Was passiert, wenn Sie die <code>min_samples</code> auf 1 bzw. 10 statt 5 setzen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Frage 3.4.23:</b> Was passiert, wenn Sie die <code>eps</code> auf 0.1 bzw. 1.0 statt 0.5 setzen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### HDBSCAN\n", "\n", "HDBSCAN ist eine Erweiterung DBSCAN, bei dem, stark vereinfacht, die Berechnung von Epsilon sowie die Anzahl an Datenpunkten pro Cluster, auf Kosten der Laufzeit, automatisiert wurde."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gegeben sei der bekannte Blob-Datensatz:"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_blobs\n", "import matplotlib.pyplot as plt\n", "RANDOM_STATE = 42\n", "X_blob, y_blob = make_blobs(\n", "    n_samples=300, centers=4,cluster_std=0.60, random_state=RANDOM_STATE, shuffle=True\n", ")\n", "plt.figure(figsize=(15,10))\n", "plt.scatter(X_blob[:, 0],X_blob[:, 1], s=50)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Aufgabe 3.4.24:</b> Verwenden Sie <code>hdbscan</code> mit den Parametern <code>min_cluster_size=5</code> (minimale Anzahl an Datenpunkten die einen Cluster bilden) sowie <code>min_samples=None</code> (Steuert die Anzahl an DAtenpunkten, die keiner Klasse zugeordnet werden k\u00f6nnen), um die Daten mittels HDBSCAN zu Clustern und speichern Sie das Ergebnis in einer Variablen mit dem Namen <code>predicted_labels</code>.\n", "    \n", "    \n", "_Hinweis: Verweden Sie <code>hdbscan.HDBSCAN</code>, welches das gleiche Interface wie die die vorherigen Methoden besitzt._\n", "    \n", "_Hinweis 2: Sollten Sie die Aufgabe lokal ausf\u00fchren, m\u00fcssen Sie die Bibliothek <code>hdbscan</code> installieren. Dies k\u00f6nnen Sie in der Anaconda Promt mittels \"conda install -c conda-forge hdbscan\" durchf\u00fchren._\n", "</div>"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["from hdbscan import HDBSCAN\n", "\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "plt.figure(figsize=(15,10))\n", "plt.scatter(\n", "    X_blob[:, 0], X_blob[:, 1], c=predicted_labels\n", ")\n", "plt.show()\n", "print('Number of classes: {}'.format(len(list(filter(lambda x: x != -1 , set(predicted_labels))))))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Frage 3.4.25:</b> Was passiert, wenn Sie die <code>min_cluster_size</code> auf 10 bzw. 100 statt 5 setzen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Frage 3.4.26:</b> Was passiert, wenn Sie die <code>min_samples</code> auf 1, 10 bzw. 100 statt 5 setzen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Praktische Anwendung - Bildkomprimierung\n", "\n", "Im folgenden sollen die genannten Algorithmen praktisch an einem Beispielbild ausprobiert werden, d.h. wir werden im folgenden die Bildkompression mittels Clustering betrachten.\n", "\n", "Das Ursprungsbild sieht wie folgt aus:"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# NICHT \u00c4NDERN\n", "from skimage import io\n", "import os\n", "import matplotlib.pyplot as plt \n", "\n", "path_to_image = os.path.join('images', 'Lamas_0.jpg')\n", "image = io.imread(path_to_image)\n", "\n", "plt.figure(figsize=(15,10))\n", "plt.imshow(image)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nun wird das gezeigte Bild mittels KMeans komprimiert.\n", "\n", "**Achtung: Die Berechnung kann etwas Zeit in anspruch nehmen.**"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["from skimage import img_as_float\n", "from sklearn.cluster import  KMeans\n", "import numpy as np\n", "\n", "# Bild-Dimensionen\n", "rows = image.shape[0]\n", "cols = image.shape[1]\n", "\n", "# Bilddimension \u00e4ndern\n", "image_reshaped = image.reshape(rows*cols, 3)\n", "\n", "# k-means\n", "model = KMeans(n_clusters=4)\n", "model.fit(image_reshaped)\n", "\n", "# neues Bild erzeugen\n", "compressed_image = model.cluster_centers_[model.labels_]\n", "compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)\n", "\n", "# In Ursprungsdimension konvertieren\n", "compressed_image = compressed_image.reshape(rows, cols, 3)\n", "\n", "plt.figure(figsize=(15,10))\n", "plt.imshow(compressed_image)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "    <b>Aufgabe 3.4.27:</b> Komprimieren Sie <code>image</code> mittels <code>MeanShift</code> \u00e4quivalent zu dem Beispiel mit <code>KMeans</code>. Verwenden Sie hierbei eine <code>bandwidth</code> von 8 und \u00fcbergeben <code>bin_seeding=True</code> (dies beschleunigt die Berechnung).  \n", "    \n", "_Hiweise: Verwenden Sie zum speichern des Cluster-Verfahrens eine Variable mit dem Namen <code>model</code>._\n", "</div>"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["from skimage import img_as_float\n", "import numpy as np\n", "\n", "# Bild-Dimensionen\n", "rows = image.shape[0]\n", "cols = image.shape[1]\n", "\n", "# Bilddimension \u00e4ndern\n", "image_reshaped = image.reshape(rows*cols, 3)\n", "\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE\n", "\n", "\n", "# neues Bild erzeugen\n", "compressed_image = model.cluster_centers_[model.labels_]\n", "compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)\n", "\n", "# In Ursprungsdimension konvertieren\n", "compressed_image = compressed_image.reshape(rows, cols, 3)\n", "\n", "plt.figure(figsize=(15,10))\n", "plt.imshow(compressed_image)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Optionale Themen\n", "\n", "Alles Nachfolgende in diesem Notebook ist optional. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Optionales Thema - Principal Component Analysis (PCA)\n", "\n", "Wie bereits erw\u00e4hnt, gibt es verschiedene Anwendungsbereiche f\u00fcr un\u00fcberwachte Lernalgorithmen.  Eine der h\u00e4ufigsten Aufgaben beim un\u00fcberwachten Lernen ist die Dimensionalit\u00e4tsreduktion. Hier ist das Ziel, die Daten zu vereinfachen, ohne zu viele Informationen zu verlieren. Mehrere korrelierende Merkmale k\u00f6nnen zu einem zusammengefasst werden. Zum Beispiel korreliert der Kilometerstand eines Autos mit seinem Alter, so dass ein Algorithmus zur Dimensionalit\u00e4tsreduktion beide zu einem Merkmal kombinieren kann, das die Abnutzung des Fahrzeugs darstellt. Dies wird auch als Merkmalsextraktion bezeichnet. Visualisierungsalgorithmen sind ein weiteres gutes Beispiel f\u00fcr un\u00fcberwachtes Lernen: Man gibt ihnen eine Menge komplexer Daten ohne Beschriftungen und erh\u00e4lt eine 2-D- oder 3-D-Darstellung der Daten, die man leicht grafisch darstellen kann. Solche Algorithmen versuchen, die Struktur der Daten so gut wie m\u00f6glich zu erhalten (z. B. verhindern sie, dass sich Cluster in den Eingabedaten in der Visualisierung \u00fcberlappen), damit Sie leichter verstehen k\u00f6nnen, wie die Daten strukturiert sind, und m\u00f6glicherweise auf unerwartete Muster sto\u00dfen.\n", "\n", "Heute werden wir die Hauptkomponentenanalyse verwenden, um Datens\u00e4tze zu visualisieren und ihre Dimensionalit\u00e4t zu reduzieren.\n", "\n", "### Intuition, Theorien und Anwendungsfragen\n", "\n", "Die Principal Component Analysis ist eine der einfachsten, intuitivsten und am h\u00e4ufigsten verwendeten Methoden zur Dimensionalit\u00e4tsreduktion, bei der Daten auf ihren orthogonalen Merkmalsunterraum projiziert werden. Orthogonaler Unterraum bedeutet, dass wir einen Merkmalsraum aus Linearkombinationen unserer urspr\u00fcnglichen Merkmale erstellen. Wie im Originalraum sind die neu konstruierten Features orthogonal zueinander. Es wird zun\u00e4chst die Hyperebene gefunden, die den Daten am n\u00e4chsten liegt, und dann werden die Daten auf diese Ebene projiziert.\n", "\n", "Aber gehen wir einen Schritt zur\u00fcck: Was ist \u00fcberhaupt ein Merkmalsraum und warum wollen wir die \"Dimensionalit\u00e4t\" unserer Daten reduzieren? In den meisten F\u00e4llen kann jeder Datenpunkt in unserem Datensatz im Wesentlichen als ein Vektor mit einer gewissen Anzahl von Elementen dargestellt werden. Der Raum, der alle diese Vektoren umfasst (z. B. $\\mathbb{R}\\times\\mathbb{R}\\times\\mathbb{R}$, wenn alle unsere Vektoren jeweils 3 Elemente haben), wird als Merkmalsraum bezeichnet. In realen F\u00e4llen werden wir oft eine gro\u00dfe Anzahl von Dimensionen haben, was Analysen (sowohl manuell als auch mit Computerunterst\u00fctzung) schwierig macht. Wenn wir einen Weg finden, die Anzahl der Dimensionen zu reduzieren, ohne Informationen zu verlieren, wird unser Leben sehr viel einfacher. Um dies in realen F\u00e4llen zu erreichen, gibt es verschiedene Algorithmen, wie z. B. die hier verwendete PCA, die Wege finden, die Merkmale (Vektorelemente) in einen neuen, deutlich kleineren Satz von Merkmalen zu transformieren, ohne dabei viel Information zu verlieren.\n", "Schlie\u00dflich bedeutet die Tatsache, dass es Linearkombinationen verwendet, nur, dass jede Komponente f\u00fcr unseren neuen Vektor aus einer gewichteten Summe der urspr\u00fcnglichen Komponenten besteht. Zum Beispiel k\u00f6nnte die erste Komponente unserer Ausgabe aus den drei Komponenten unseres Eingabevektors \u00fcber $(x^{\\text{pca}})_0 = 0,27 * (x^{\\text{input}})_0 - 4,21 * (x^{\\text{input}})_1 + 1,31 * (x^{\\text{input}})_2$ gebildet werden. Das Sch\u00f6ne daran ist, dass diese Transformationen unglaublich schnell zu berechnen sind und als eine einzige Matrixmultiplikation ausgedr\u00fcckt werden k\u00f6nnen.\n", "\n", "<table class=\"image\">\n", "<caption align=\"bottom\"> Auswahl eines Subraums f\u00fcr die Projektion [Geron, 2018]</caption>\n", "<tr><td><img src='images/pca_projection_geron.png' width=70% /></td></tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Als Beispiel zeigt die obige Abbildung auf der linken Seite einen einfachen 2-D-Datensatz mit drei verschiedenen Achsen (d.h. eindimensionalen Hyperebenen). Auf der rechten Seite sehen Sie die Projektionen des Datensatzes auf jede dieser Achsen. Wie Sie sehen k\u00f6nnen, beh\u00e4lt die Projektion auf der durchgezogenen Linie die maximale Varianz, w\u00e4hrend die Projektion auf der gestrichelten Linie sehr wenig Varianz beh\u00e4lt. Die Projektion auf der gestrichelten Linie liegt irgendwo dazwischen.\n", "\n", "Es erscheint sinnvoll, die Achse so zu w\u00e4hlen, dass ein Maximum an Varianz erhalten bleibt, da weniger Informationen verloren gehen als bei den anderen Projektionen. Die Auswahl l\u00e4sst sich auch damit begr\u00fcnden, dass der mittlere quadratische Abstand zwischen dem urspr\u00fcnglichen Datensatz und der Projektion minimal ist. Dieser einfache Grundgedanke ist die Essenz der Principal Component Analysis: Die PCA versucht, durch Minimierung des Rekonstruktions- (Projektions-) Fehlers die erste Hauptkomponente zu finden, die gleichzeitig die Varianz der projizierten Daten maximiert.\n", "\n", "<img align=right src='images/pca_projection_connection.png' width=\"400\"/>\n", "\n", "Die PCA sucht nach der Achse, auf der die gr\u00f6\u00dfte Varianz der Trainingsdaten liegt. Es wird auch eine Achse orthogonal zur ersten Achse gefunden, die der gr\u00f6\u00dften verbleibenden Varianz entspricht. In einem h\u00f6herdimensionalen Datensatz w\u00fcrde die PCA auch eine dritte Achse finden, die orthogonal zu den beiden vorherigen Achsen ist, dann eine vierte Achse, eine f\u00fcnfte Achse und so weiter - so viele Achsen, wie der Datensatz Dimensionen hat. Der Einheitsvektor, der die $i^{th}$-Achse definiert, wird $i^{th}$ Hauptkomponente (PC) genannt. In der Abbildung ist die 1. Hauptkomponente $c_1$ und die zweite $c_2$.\n", "\n", "Allgemeiner ausgedr\u00fcckt k\u00f6nnen alle Beobachtungen als ein Ellipsoid in einem Unterraum eines anf\u00e4nglichen Merkmalsraums betrachtet werden, und die neue Basismenge in diesem Unterraum wird an den Ellipsoidachsen ausgerichtet. Diese Annahme erm\u00f6glicht es, stark korrelierte Merkmale zu entfernen, da die Vektoren der Basismengen orthogonal sind. \n", "Im allgemeinen Fall stimmt die resultierende Ellipsoid-Dimensionalit\u00e4t mit der Dimensionalit\u00e4t des Ausgangsraums \u00fcberein, aber die Annahme, dass unsere Daten in einem Unterraum mit einer kleineren Dimension liegen, erlaubt es uns, den \"\u00fcberm\u00e4\u00dfigen\" Raum mit der neuen Projektion (Unterraum) abzuschneiden. Wir erreichen dies auf eine \"gierige\" Art und Weise, indem wir nacheinander jede der Ellipsoid-Achsen ausw\u00e4hlen, indem wir feststellen, wo die Streuung (Varianz) maximal ist.\n", " \n", "\n", "> \"To deal with hyper-planes in a 14 dimensional space, visualize a 3D space and say 'fourteen' very loudly. Everyone does it.\" - Geoffrey Hinton"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Werfen wir einen Blick auf die mathematische Formulierung dieses Prozesses:\n", "\n", "Um die Dimensionalit\u00e4t unserer Daten von $n$ auf $k$ mit $k \\leq n$ zu verringern, sortieren wir unsere Liste von Achsen in der Reihenfolge abnehmender Streuung und nehmen die obersten $k$ davon.\n", "\n", "Aber wie kann ich die Principal Components f\u00fcr einen Trainingsdatensatz finden?\n", "\n", "Wir beginnen mit der Berechnung der Varianz und der Kovarianz der urspr\u00fcnglichen Features. Dies wird normalerweise mit der Kovarianzmatrix durchgef\u00fchrt. Gem\u00e4\u00df der Kovarianzdefinition wird die Kovarianz zweier Features wie folgt berechnet: $$cov(X_i, X_j) = E[(X_i - \\mu_i) (X_j - \\mu_j)] = E[X_i X_j] - \\mu_i \\mu_j,$$ wobei $\\mu_i$ der Erwartungswert des $i$-ten Merkmals ist. Es ist erw\u00e4hnenswert, dass die Kovarianz symmetrisch ist, und die Kovarianz eines Vektors mit sich selbst gleich seiner Varianz ist.\n", "\n", "Daher ist die Kovarianzmatrix symmetrisch mit der Varianz der entsprechenden Merkmale auf der Diagonalen. Nicht-diagonale Werte sind die Kovarianzen des entsprechenden Merkmalspaares. In Bezug auf Matrizen, bei denen $\\mathbf{X}$ die Matrix der Beobachtungen ist, lautet die Kovarianzmatrix wie folgt:\n", "\n", "$$\\mathbf{C} = E[(\\mathbf{X} - E[\\mathbf{X}]) (\\mathbf{X} - E[\\mathbf{X}])^{T}]$$\n", "\n", "Im Fall von zwei Dimensionen:\n", "\n", "$$ \\Large \\mathbf{C} = \\begin{pmatrix}\n", "\\sigma_{X_1}^2 & cov(X_1, X_2)    \\\\\n", " cov(X_2, X_1) & \\sigma_{X_2}^2   \\\\\n", "\\end{pmatrix} $$\n", "Wenn wir einen mittelwertzentrierten Datensatz haben ($\\mu =0$), vereinfacht sich die Berechnung zu \n", "\n", "$\\Large \\mathbf{C} = \\frac{\\mathbf{X}\\cdot \\mathbf{X}^T}{n-1}$.\n", "\n", "Erinnern Sie sich nun daran, was die PCA erreichen will: Maximierung der Varianz im projizierten Raum. Genauso gesprochen:\n", "Wenn $\\mathbf{X}$ die Datenmatrix ist, dann ist die Projektion auf den Vektor $w$ gegeben durch $\\mathbf{X}w$ und ihre Varianz ist \n", "$$\\Large \\sigma_w^2 =  \\frac{1}{n\u22121}(\\mathbf{X}w)^T \\cdot\\mathbf{X}w=w^T\\cdot (\\frac{1}{n\u22121}\\mathbf{X}^T\\mathbf{X})\\cdot w=w^T \\mathbf{C} w$$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Kurze Zusammenfassung: Matrizen, als lineare Operatoren, haben Eigenwerte und Eigenvektoren. Sie sind sehr praktisch, weil sie Teile unseres Raums beschreiben, die nicht rotieren und sich nur dehnen, wenn wir lineare Operatoren auf sie anwenden; Eigenvektoren bleiben in der gleichen Richtung, werden aber um einen entsprechenden Eigenwert gedehnt. Formal erf\u00fcllt eine Matrix $\\mathbf{M}$ mit Eigenvektor $v_i$ und Eigenwert $\\lambda_i$ diese Gleichung: $\\mathbf{M} v_i = \\lambda_i v_i$.\n", "Da unsere Kovarianzmatrix $ \\mathbf{C} $ symmetrisch ist, k\u00f6nnen wir sie diagonalisieren, indem wir ein neues orthogonales Koordinatensystem w\u00e4hlen, das durch die Eigenvektoren der Kovarianzmatrix $ \\mathbf{C} $ gegeben ist. Die Kovarianzmatrix im neuen Raum sieht so aus:\n", "$$ \\Large \\mathbf{C_{EV}} = \\begin{pmatrix}\n", "\\lambda_1 & 0  \\\\\n", "0 & \\lambda_2  \\\\\n", "\\end{pmatrix} $$\n", "Die neue Kovarianzmatrix ist also diagonal, die Dimensionen sind unkorreliert ($ cov(EV_1, EV_2) = 0 $) und die Eigenwerte $\\lambda_i$ liegen auf der Diagonalen. Setzt man dies oben in $\\sigma_w^2$ ein (wir wollen das maximieren), ergibt sich:\n", "$$ \\Large \\sigma_w^2 =  w^T \\mathbf{C}_{EV} w = \\Sigma_i \\lambda_i w^2_i$$\n", "In der neuen orthogonalen Basis ist die Varianz durch die gewichtete Summe der Eigenwerte gegeben. Folglich wird die maximal m\u00f6gliche Varianz erreicht, wenn wir einfach die Projektion auf die erste Koordinatenachse nehmen. Daraus folgt, dass die Richtung der ersten Principal Components durch den ersten Eigenvektor der Kovarianzmatrix gegeben ist.\n", "Fasst man alle Eigenvektoren in einer Matrix zusammen, erh\u00e4lt man\n", "$$\\Large \\mathbf{V} = \\begin{pmatrix}\n", "\\mid & \\mid & & \\mid \\\\\n", "{\\bf v}_{1} & {\\bf v}_{2} & \\cdots & {\\bf v}_{n}\\\\\n", "\\mid & \\mid & & \\mid \\\\\n", "\\end{pmatrix}$$\n", "Die Principal Components, die wir aus den Daten erhalten wollen, sind nur die Eigenvektoren, die den obersten-$k$ gr\u00f6\u00dften Eigenwerten der Matrix entsprechen. Die PCA setzt voraus, dass der Datensatz um den Koordinatenursprung zentriert ist, was notwendig ist, um zu gew\u00e4hrleisten, dass die erste Principal Components (erster Eigenvektor) in der Achse mit maximaler Varianz liegt. Andernfalls kann die erste Principal Components nur auf den Mittelwert des Datensatzes zeigen. \n", "\n", "Die n\u00e4chsten Schritte sind einfacher zu verdauen. Wir multiplizieren die Matrix unserer Daten $\\mathbf{X}$ mit diesen Komponenten, um die Projektion unserer Daten auf die orthogonale Basis der gew\u00e4hlten Komponenten zu erhalten. Wenn die Anzahl der Komponenten kleiner war als die anf\u00e4ngliche Raumdimensionalit\u00e4t, denken Sie daran, dass wir bei Anwendung dieser Transformation einige Informationen verlieren werden.\n", "\n", "Wenn wir die Mathematik von oben als Pseudocode formulieren, erhalten wir die"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "\n", "\n", "\n", "\n", "**Pseudocode f\u00fcr PCA** \n", "\n", ". Definieren Sie $k<d$ - als neue Dimensionalit\u00e4t\n", "2. Mittelwert-Zentrierung von $X$ durchf\u00fchren: \n", " \n", "3. Berechnen Sie die Kovarianzmatrix $\\mathbf{C}$ von $\\mathbf{X}$\n", "\n", "4. F\u00fchren Sie die Eigenwertzerlegung der Kovarianzmatrix so durch, dass:\n", "    $$ \\mathbf{C} \\cdot v_i = \\lambda_i \\cdot v_i $$\n", "    \n", "5. Sortieren Sie die Eigenvektoren $v_i$ in absteigender Reihenfolge ihrer Eigenwerte $\\lambda_i$, um $V \\in \\mathbb{R}^{d\\times d}$ zu erhalten\n", "\n", "6. W\u00e4hlen Sie die obersten-$k$ Eigenvektoren als Hauptkomponenten\n", " $$V = \\begin{pmatrix}\n", "v_{1,1} & \\cdots & v_{1,k} \\\\\n", " \\vdots & & \\vdots \\\\\\\n", " v_{d,1} & &v_{d,k} \\\\\n", "\\end{pmatrix} \\in \\mathbb{R}^{d \\times k} $$\n", "6. R\u00fcckgabe $$\\Large Z = XV \\in \\mathbb{R}^{m \\times k}$$\n", "7. Verlust gegeben durch \n", "  - $\\Large \\sum_{i=k+1}^n \\lambda_i$ (\"verlorene\" Varianz) oder\n", "  \n", "  - $\\lVert \\mathbf{X} - \\mathbf{X}ww^T\\rVert^2$ (Reprojektionsfehler)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Entwickeln einer PCA-Implementierung\n", "\n", "In dieser Aufgabe wollen wir eine eigene einfache PCA-Implementierung realisieren und das Funktionsprinzip verstehen.\n", "\n", "#### Verstehen der PCA mit dem Iris-Datensatz\n", "Beginnen wir mit dem Datensatz, den wir in Aufgabe 2 verwendet haben, dem Iris-Datensatz. Dazu m\u00fcssen wir zun\u00e4chst alle ben\u00f6tigten Bibliotheken und unsere Daten laden. "]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["# Imports\n", "import numpy as np\n", "import pandas as pd\n", "from scipy.stats import pearsonr\n", "from sklearn.preprocessing import LabelEncoder\n", "import matplotlib.pyplot as plt\n", "from sklearn import decomposition\n", "from IPython import display\n", "%matplotlib inline\n", "import seaborn as sns\n", "import ipywidgets as widgets\n", "\n", "plt.style.use(['seaborn-darkgrid'])\n", "plt.rcParams['figure.figsize'] = (12, 9)\n", "plt.rcParams['font.family'] = 'DejaVu Sans'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Second, we load the [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) flower data set from the csv file, just as in task 2. The sepal length is shown versus the petal width of the flowers."]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["# Load dataset\n", "filename = 'data/iris.data.csv'\n", "names = ['Sepal-length', 'Sepal-width', 'Petal-length', 'Petal-width', 'Class']\n", "X = pd.read_csv(filename, names=names)\n", "# split label info\n", "y = X.Class\n", "# drop labels from data\n", "X = X.drop(['Class'],axis=1)\n", "plt.scatter(X['Sepal-length'],X['Petal-width'])\n", "plt.xlabel('Sepal Length')\n", "plt.ylabel('Petal width')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "<b>Hinweis:</b> Wie Sie vielleicht sehen, haben wir Zielwerte (aka Labels) f\u00fcr unsere Daten, also k\u00f6nnten wir auch \u00fcberwachte Methoden verwenden. Aber wir k\u00f6nnen die Labels auch im un\u00fcberwachten Kontext verwenden, was wir sp\u00e4ter sehen werden.\n", "\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Der n\u00e4chste Schritt in unserem Analyseprozess w\u00e4re eine Erkundung unseres Datensatzes mit verschiedenen Statistiken und Diagrammen. Da wir dies in Aufgabe 3 mit dem Titanic-Datensatz bereits getan haben, \u00fcberspringen wir diesen Schritt."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Danach k\u00f6nnen wir unsere Daten f\u00fcr die Verwendung der PCA vorbereiten. Nach Schritt zwei des Pseudocodes der PCA ist es notwendig, unsere Daten zu zentrieren."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> F\u00fcllen Sie die folgenden Zeilen aus, um den Vorbereitungsschritt durchzuf\u00fchren. Benennen Sie den <strong>zentrierten</strong> Datensatz \n", "    <strong>X_centered</strong>.\n", "\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": ["# center X by mean\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Wenn Sie das Diagramm von oben betrachten, vermuten Sie, dass die Kelchblattl\u00e4nge und die Bl\u00fctenblattbreite korreliert sind."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Um dies zu best\u00e4tigen, berechnen Sie ihre Pearson-Korrelation.\n", "\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Bekommen Sie einen Beweis f\u00fcr Ihre Aussage, wenn Sie sich den Korrelationswert ansehen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Um eine bessere Vorstellung von der PCA zu bekommen, wollen wir zun\u00e4chst das Funktionsprinzip zeigen. Deshalb haben wir ein interaktives Widget erstellt. Das Widget zeigt ein Scatterplot der Kelchblattl\u00e4nge und der Bl\u00fctenblattbreite unseres Datensatzes. Die Daten werden auf die ausgew\u00e4hlte Achse projiziert, die als schwarze durchgezogene Linie dargestellt wird. Die Achse der Projektion kann mit dem Schieberegler oberhalb des Bildes eingestellt werden, der den Winkel der Linie steuert. Der Verlust der Projektion (Reprojektionsfehler) wird ebenfalls berechnet."]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": ["#### DO NOT EDIT\n", "def projectionError(pointsOrig,projectionMatrix):\n", "    pointsProjected = pointsOrig.dot(projectionMatrix).dot(projectionMatrix.T)\n", "    sumSquaredDistance = sum(sum((pointsProjected-pointsOrig)**2))\n", "    projErr = sumSquaredDistance/len(pointsOrig)\n", "    return projErr\n", "def varianceProjected(pointsOrig, projectionMatrix):\n", "    z = pointsOrig.dot(projectionMatrix)\n", "    return np.var(z[:,0])\n", "def printLossGain(pointsOrig,projectionMatrix):\n", "    print(\"Projection Error: {:0.4f}\".format(projectionError(pointsOrig,projectionMatrix)))\n", "    print(\"Projected Variance: {:0.4f}\".format(varianceProjected(pointsOrig,projectionMatrix)))"]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": ["#### DO NOT EDIT\n", "###PCA###drawing\n", "def update_pca_2D(X,alpha):\n", "    #plt.figure(2,figsize=(8,8))\n", "    plt.cla()\n", "    w = np.array([np.cos(np.deg2rad(alpha)), np.sin(np.deg2rad(alpha))]).reshape(2,1)\n", "    z = X.dot(w).dot(w.T)\n", "    for i in range(0,100):\n", "        plt.plot([X[i,0], z[i,0]], [X[i,1], z[i,1]], c='r',aa=False)\n", "    \n", "    plt.plot(w[0]*3.5*np.array([-1, 1]), w[1]*3.5*np.array([-1, 1]), c='k',aa=False)\n", "    plt.plot(-w[1]*2*np.array([-1, 1]), w[0]*2*np.array([-1, 1]), c=(.6 ,.6, .6),aa=False)\n", "    \n", "    plt.scatter(z[:,0], z[:,1], c='r')\n", "    plt.scatter(X[:,0], X[:,1], c='b')\n", "    sct = plt.scatter(0,0,65,c='k',linewidth= 2)\n", "    #plt.setp(sct,mfc = (1, 1 ,1), mec = (0, 0, 0))    \n", "    printLossGain(X,w)\n", "    minval = min(min(X[:,0]),min(X[:,1]))\n", "    maxval = max(max(X[:,0]),max(X[:,1]))\n", "    plt.xlim([minval, maxval])\n", "    plt.ylim([minval, maxval])\n", "    plt.xlabel('Sepal Length')\n", "    plt.ylabel('Petal width')\n", "    #plt.show()"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["#### DO NOT EDIT\n", "###PCA###drawing\n", "\n", "plt.figure() \n", "X0 = X_centered[['Sepal-length','Petal-width']]\n", "interactive_plot = widgets.interactive(update_pca_2D,\n", "         alpha=widgets.FloatSlider(min=-90, max=90, step=0.2, layout=widgets.Layout(width='90%')),\n", "                               X=widgets.fixed(X0.values))\n", "output = interactive_plot.children[-1]\n", "output.layout.height = '600px'\n", "interactive_plot"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Stellen Sie nun den Wert von alpha so ein, dass die Varianz maximiert wird. Was bemerken Sie bez\u00fcglich des Projektionsfehlers, und welchen Winkel erhalten wir f\u00fcr die schwarze Linie?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Wo in der Abbildung ist unsere erste Principal Component angegeben? Nennen Sie eine m\u00f6gliche erste Hauptkomponente. Was zeigt die graue Linie an?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Erl\u00e4utern Sie den Verlust und wie wir hier einen Verlust machen. Was ist der Zusammenhang zwischen der ersten Principal Component und der Varianz?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Implementieren der PCA-Funktion.\n", "Verwenden Sie den angegebenen Pseudocode, um eine PCA durchzuf\u00fchren. F\u00fcllen Sie dazu die L\u00fccken im nachfolgenden Code."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Verwenden Sie den oben angegebenen Pseudocode (siehe Pseudocode f\u00fcr PCA) in dieser Aufgabe, um eine PCA durchzuf\u00fchren. Erledigen Sie dazu die hier angegebenen Teilaufgaben.\n", "<ul>\n", "<li> Berechnen Sie die Kovarianzmatrix. Sehen Sie sich die Definition von <code>np.cov()</code> an. Welche Operation ist notwendig?\n", "<li> F\u00fchren Sie die Eigenwertzerlegung durch\n", "<li> Sortieren Sie die Spalten von V mit den Eigenwerten w. Tipp: Verwenden Sie die Methode <code>.argsort()</code>, um die Indizes f\u00fcr die Sortierung von V und w zu erhalten\n", "<li> Berechnen Sie den Verlust \u00fcber Eigenwerte\n", "</li>\n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": ["def pca_own(X,k): \n", "    \"\"\" Calculates the Principal Component Analysis of X\n", "        Args: \n", "            X : correctly scaled matrix of data, each row is an observation\n", "            k (int) : number of components selected for the output.\n", "        Returns: \n", "            w : k eigenvalues \n", "            V : the matrix of k principal components\n", "            loss: the error made by using only k components as feature space\n", "    \"\"\"\n", "    # Tip: use the argsort() function to get the indices for sorting of V\n", "    # STUDENT CODE HERE   \n", "\n", "    # STUDENT CODE until HERE\n", "    return w_sorted[:k], V_sorted[:,:k], loss\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Benutzen Sie nun Ihre Funktion, um die beiden PCs des Datensatzes $X0$ zu berechnen. $X0$ besteht aus den beiden Merkmalen Kelchblattl\u00e4nge und Bl\u00fctenblattbreite. Die Funktion zum Plotten ist bereits unten angegeben.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["# select features for demonstration\n", "X0 = X_centered[['Sepal-length','Petal-width']]\n", "# Call your PCA function with given k\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "code", "execution_count": 31, "metadata": {"scrolled": true}, "outputs": [], "source": ["# DO NOT CHANGE\n", "print('Directions of principal components:\\n' +\\\n", "      '1st component:',\n", "      V[:,0],\n", "      '\\n2nd component:', \n", "      V[:,1]\n", "     )\n", "print('Loss by PCA transformation: ',loss)\n", "\n", "\n", "# Plotting function\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(X0['Sepal-length'], X0['Petal-width'], s=50, c='b')\n", "\n", "# Transpose matrix for plotting purposes\n", "V_transp = V.T\n", "\n", "for l, v in  zip(w, V_transp):\n", "    d = 5 * np.sqrt(l) * v\n", "    plt.plot([0, d[0]], [0, d[1]], '-k', lw=3)\n", "    \n", "plt.axis('equal')\n", "\n", "plt.xlabel('Sepal Length')\n", "plt.ylabel('Petal width')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Schauen Sie sich den oben berechneten Verlust an. Haben Sie eine Erkl\u00e4rung f\u00fcr den Wert?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Reduzieren der Dimension\n", "Bis jetzt haben wir die Richtung der ersten beiden Hauptkomponenten des Datensatzes $X0$ gesehen. Nun wollen wir den gesamten Datensatz ($X_{zentriert}$) auf zwei Variablen reduzieren."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Rufen Sie Ihre eigene Funktion \"pca_own\" mit dem richtigen Wert f\u00fcr $k$ auf. Anschlie\u00dfend transformieren Sie die Daten in den <strong>neuen</strong> Merkmalsraum unter Verwendung der von Ihrer Funktion zur\u00fcckgegebenen Principal Components und nennen die transformierten Daten <strong>X_transf</strong>.\n", "\n", "\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "<b>Hinweis:</b> Wenn Sie bei Ihrer eigenen pca und der scikit-learn-Implementierung unterschiedliche Plots erhalten, kann dies daran liegen, dass Eigenvektoren bei dieser Transformation einen Vorzeichenwechsel haben oder haben k\u00f6nnen. Siehe auch folgende Diskussionen:  <a href=\"https://stackoverflow.com/questions/44765682/in-sklearn-decomposition-pca-why-are-components-negative\">\u00c4nderung des Vorzeichens</a>  und <a href=\"https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\">SVD w.r.t. PCA</a>\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Die Funktion zur Visualisierung der Ergebnisse ist unten angegeben. Die Farbe der Punkte gibt den Zielwert (=Iris-Klasse) an."]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["# DO not edit\n", "print(loss)\n", "print('Directions of principal components:\\n' +\\\n", "      '1st component:', \n", "      V[:,0],\n", "      '\\n2nd component:',\n", "      V[:,1]\n", "     )\n", "\n", "print(\"Meaning of the 2 components:\")\n", "for component in V.T:\n", "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n", "                     for value, name in zip(component,\n", "                                            names)))\n", "plt.figure(figsize=(10,7))\n", "# encoding labels as numbers for plotting\n", "le = LabelEncoder()\n", "y_plt = le.fit_transform(y)\n", "#plt.scatter(X_transf.loc[:, 0], X_transf.loc[:, 1], c=y_plt, s=70, cmap='viridis')\n", "plt.scatter(X_transf.loc[:, 0], X_transf.loc[:, 1], c=y_plt, s=70, cmap='viridis')\n", "\n", "plt.xlabel('PC 1')\n", "plt.ylabel('PC 2')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Welchen Wert m\u00fcssen wir f\u00fcr k einstellen, und welchen Nutzen hat die PCA in diesem Fall?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Welche Bedeutung haben die beiden Principal Components, wie oben abgedruckt? \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "### PCA f\u00fcr Iris-Datensatz mit scikit-learn \n", "Nachdem wir nun PCA von Grund auf implementiert und die wichtigsten Eigenschaften verstanden haben, k\u00f6nnen wir einen Schritt weitergehen und die eingebaute PCA von Scikit-learn verwenden. Es wird leicht abweichende Ergebnisse im Vergleich zu unserer eigenen PCA-Funktion geben, da die Scikit-Learn-Implementierung eine andere mathematische Formulierung des Problems verwendet. Details dazu finden Sie im Anhang. Andererseits enth\u00e4lt die Implementierung von Scikit weitere hilfreiche Funktionen. Zum Beispiel k\u00f6nnen wir mit der Funktion fit_transform unsere Daten direkt in den gew\u00e4hlten Unterraum transformieren. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Reduzieren von Dimensionen\n", "Die Bibliothek \"decomposition\" von sklearn enth\u00e4lt die PCA-Implementierung."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Berechnen Sie die PCA des vollst\u00e4ndigen zentrierten Datensatzes $\\mathbf{X}_{centered}$ und w\u00e4hlen Sie zwei Principal Components mit Hilfe der Zerlegungsbibliothek aus. Anschlie\u00dfend transformieren Sie den Datensatz in den neuen Merkmalsraum und geben die Werte der beiden Komponenten aus. F\u00fcllen Sie die L\u00fccken unten aus und erhalten Sie die PCs.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE \n", "\n", "# STUDENT CODE until HERE "]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["print('Direction of first principal component:\\n' ,\n", "      # STUDENT CODE HERE \n", "\n", "      # STUDENT CODE until HERE \n", "     )"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["print('Direction of second principal component:\\n' ,\n", "      # STUDENT CODE HERE \n", "\n", "      # STUDENT CODE until HERE \n", "     )"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,7))\n", "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_plt, s=70, cmap='viridis')\n", "plt.xlabel('PC 1')\n", "plt.ylabel('PC 2')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Wie wir oben gesehen haben, modelliert jeder PC einen Teil der Varianz unserer Originaldaten. Wie viel von der urspr\u00fcnglichen Dateninformation bleibt erhalten, wenn wir die Dimensionen reduzieren? Welcher Anteil der Varianz geht verloren, wenn wir nur einen PC verwenden w\u00fcrden? Zeigen/drucken Sie daher die entsprechenden Parameter des PCA-Modells, das von scikit-learn angepasst wurde, und beantworten Sie diese Frage.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Im Allgemeinen sollte die Menge an Informationen, die wir hier behalten, ausreichen, um eine gute Darstellung unseres Datensatzes zu erhalten. In der Praxis w\u00fcrden wir die Anzahl der Principal Components so w\u00e4hlen, dass wir mindestens __90% der anf\u00e4nglichen Datenstreuung__ erkl\u00e4ren k\u00f6nnen (\u00fcber das `explained_variance_ratio`). \n", "\n", "#### Kumulierte Abweichung\n", "Nachfolgend wird eine Funktion zum Plotten des individuellen erkl\u00e4rten Varianzverh\u00e4ltnisses jedes PC angegeben."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> F\u00fcgen Sie ein Diagramm f\u00fcr die kumulierte Varianz aller PCs hinzu. Die Funktion *\"step \"* mit der Option *where='mid'* wird verwendet, um die kumulative Varianz darzustellen. F\u00fcllen Sie die entsprechende L\u00fccke aus, indem Sie die kumulierte erkl\u00e4rte Varianz berechnen und sie \"cum_var_exp\" nennen.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": ["def plotPCAVariance(var_exp, X):\n", "    #STUDENT CODE HERE\n", "\n", "    #STUDENT CODE until HERE\n", "    \n", "    #plotting individual explained variance is given\n", "    fig = plt.figure(figsize=(10,7))\n", "    ax = fig.add_axes([0.1, 0.1, 0.9, 0.9])\n", "    ax.bar(range(1, X.shape[1]+1), var_exp, alpha=0.5, align='center',\n", "        label='individual explained variance')\n", "    ax.set_ylabel('Explained variance ratio')\n", "    ax.set_xlabel('Principal component index')    \n", "    ax.set_xlim(0.5, X.shape[1]+0.5)\n", "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n", "    ax.set_xticks(np.arange(1,X.shape[1]+1,1))\n", "\n", "    # plotting\n", "    ax.step(range(1, X.shape[1]+1), cum_var_exp, where='mid',\n", "         label='cumulative explained variance')\n", "    ax.legend(loc='best')\n", "    return fig, ax"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Wir m\u00fcssen die erkl\u00e4rten Varianzen f\u00fcr alle PCs erhalten, also geben wir k nicht an, wenn wir die Funktion PCA().fit() aufrufen. Anschlie\u00dfend k\u00f6nnen wir das Ergebnis als Eingabe f\u00fcr unsere Funktion verwenden und die erkl\u00e4rten Varianzen plotten."]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["#  fit to centered data with k=Number of Features\n", "pca = decomposition.PCA().fit(X_centered)\n", "# get explained variance\n", "var_exp = pca.explained_variance_ratio_\n", "#plot\n", "fig, ax = plotPCAVariance(var_exp, X_centered)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Wie viele PCs m\u00fcssen wir w\u00e4hlen, um mindestens 90% der anf\u00e4nglichen Datenstreuung zu erkl\u00e4ren? Was bedeutet das f\u00fcr unseren Merkmalsraum? Was ist die intrinsische Dimension des Datensatzes?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Einfluss der Skalierung\n", "\n", "Bis jetzt haben wir immer den mittelwertzentrierten Datensatz verwendet. Aber wie wir in fr\u00fcheren Aufgaben gelernt haben, gibt es verschiedene M\u00f6glichkeiten, unseren Datensatz zu skalieren, bevor wir eine Aktion durchf\u00fchren. Ein Beispiel ist die Normalisierung alias Z-Standardisierung."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Was glauben Sie, w\u00fcrde passieren, wenn wir PCA auf Z-standardisierte Daten anstelle von mittelwertzentrierten Daten anwenden? Erkl\u00e4ren Sie kurz, warum.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Principal Component Analyse Zusammenfassung\n", "\n", "In diesem Abschnitt haben wir die Verwendung der Principal Component Analyse zur Dimensionalit\u00e4tsreduktion, zur Visualisierung hochdimensionaler Daten und zur Merkmalsauswahl innerhalb hochdimensionaler Daten diskutiert.\n", "Aufgrund der Vielseitigkeit und Interpretierbarkeit der PCA hat sie sich in einer Vielzahl von Kontexten und Disziplinen als effektiv erwiesen.\n", "Bei jedem hochdimensionalen Datensatz neigen wir dazu, mit der PCA zu beginnen, um die Beziehung zwischen den Punkten zu visualisieren, die Hauptvarianz in den Daten zu verstehen und die intrinsische Dimensionalit\u00e4t zu verstehen (indem wir das Verh\u00e4ltnis der erkl\u00e4rten Varianz aufzeichnen).\n", "Sicherlich ist die PCA nicht f\u00fcr jeden hochdimensionalen Datensatz n\u00fctzlich, aber sie bietet einen einfachen und effizienten Weg, um einen Einblick in hochdimensionale Daten zu erhalten.\n", "\n", "Die Hauptschw\u00e4che der PCA ist, dass sie dazu neigt, stark von unterschiedlichen Skalierungen sowie Ausrei\u00dfern in den Daten beeinflusst zu werden.\n", "Aus diesem Grund wurden viele robuste Varianten der PCA entwickelt, von denen viele darauf abzielen, iterativ Datenpunkte zu verwerfen, die durch die anf\u00e4nglichen Komponenten schlecht beschrieben sind.\n", "Scikit-Learn enth\u00e4lt eine Reihe interessanter Varianten der PCA, darunter ``RandomizedPCA`` und ``SparsePCA``, beide ebenfalls im Submodul ``sklearn.decomposition``.\n", "``RandomizedPCA`` verwendet ein nicht-deterministisches Verfahren zur schnellen Approximation der ersten paar Principal Components in sehr hochdimensionalen Daten. Es hat eine Berechnungszeitkomplexit\u00e4t von $\\mathcal{O} (m \\cdot d^2) + \\mathcal{O} (d^3)$ statt $\\mathcal{O}(m \\cdot n^2) + \\mathcal{O}(n^3)$ und ist damit den bisherigen Algorithmen weit \u00fcberlegen, sofern d deutlich kleiner als n ist. w\u00e4hrend ``SparsePCA`` einen Regularisierungsterm einf\u00fchrt, der dazu dient, die Sparsamkeit der Komponenten zu erzwingen.\n", "\n", "Eine Schwierigkeit bei der Standardimplementierung der PCA (\u00fcber SVD; siehe unten) ist, dass der gesamte Trainingsdatensatz in den Speicher passen muss, damit der SVD-Algorithmus ausgef\u00fchrt werden kann. Gl\u00fccklicherweise wurden inkrementelle PCA-Algorithmen (IPCA) entwickelt: Sie k\u00f6nnen den Trainingsdatensatz in kleinere Teile aufteilen und einen IPCA-Algorithmus mit einem Teil nach dem anderen f\u00fcttern. Dies ist n\u00fctzlich f\u00fcr gro\u00dfe Trainingsdatens\u00e4tze und bei der Verwendung von PCA in Online-Umgebungen (d. h. sobald neue Datenpunkte eintreffen). Beachten Sie, dass Sie f\u00fcr jeden Teilsatz die Methode partial_fit() anstelle von fit() mit allen Trainingsdaten aufrufen m\u00fcssen."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Optionales Thema - PCA und Clustering mit Samsung Human Activity Recognition Datensatz\n", "\n", "In diesem Teil der Aufgabe arbeiten wir mit dem Datensatz [Samsung Human Activity Recognition](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones). Der Datensatz befindet sich im Ordner \"data\". Die Daten stammen von Beschleunigungssensoren und Gyros von Samsung Galaxy S3 Mobiltelefonen (mehr Infos zu den Funktionen finden Sie unter dem obigen Link). Die Art der Aktivit\u00e4t einer Person mit dem Telefon in der Tasche ist ebenfalls bekannt - ob sie ging, stand, lag, sa\u00df oder die Treppe hinauf oder hinunter ging.\n", "\n", "Zun\u00e4chst tun wir so, als sei uns die Art der Aktivit\u00e4t unbekannt, und wir versuchen, die Personen rein auf der Basis der verf\u00fcgbaren Features zu clustern. Dann l\u00f6sen wir das Problem der Bestimmung der Art der k\u00f6rperlichen Aktivit\u00e4t als ein Klassifikationsproblem."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Vorbereitung\n", "\n", "Wir importieren die notwendigen Bibliotheken Pandas, Seaborn, Numpy. Aus scikit-learn importieren wir die Metriken, KMeans, PCA, GridSearchCV, StandardScaler und einige andere notwendige Importe und Definitionen."]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "from sklearn import metrics\n", "from sklearn.cluster import KMeans\n", "from sklearn.decomposition import PCA\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "import os\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn import neighbors\n", "%matplotlib inline\n", "plt.style.use(['seaborn-darkgrid'])\n", "plt.rcParams['figure.figsize'] = (12, 9)\n", "plt.rcParams['font.family'] = 'DejaVu Sans'\n", "\n", "#Define a fixed random state\n", "RANDOM_STATE = 17"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Importieren Sie Ihren Datensatz\n", "\n", "Lassen Sie uns mit dem Importieren unseres Datensatzes fortfahren."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b>  Laden Sie sowohl die Trainings- als auch die Testdaten sowie die entsprechenden Beschriftungen. Tipp: Numpy hat eine Funktion zum Laden von Textdateien. Laden Sie die Beschriftungen als Integer-Typ.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Pr\u00fcfen Sie, ob die Dimensionen des Datensatzes korrekt sind."]}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [], "source": ["# Checking dimensions\n", "assert(X_train.shape == (7352, 561) and y_train.shape == (7352,))\n", "assert(X_test.shape == (2947, 561) and y_test.shape == (2947,))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Daten verstehen\n", "\n", "Nachdem wir nun die Daten geladen haben, wollen wir uns einen schnellen \u00dcberblick \u00fcber unseren Datensatz verschaffen. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Wie viele Beobachtungen und Features haben wir? Wie viele der Features sind numerisch? Gibt es fehlende Werte?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nun laden wir die Feature-Namen aus der Datei \"features.txt\". F\u00fcr das Clustering ben\u00f6tigen wir keinen Zielvektor. Daher arbeiten wir am Anfang mit der Kombination von Trainings- und Testdatens\u00e4tze. Der folgende Code verschmilzt `X_train` mit `X_test` und `y_train` mit `y_test`. Anschlie\u00dfend werden die zusammengef\u00fchrten Daten `X_train` mit `X_test` in einen Datenrahmen mit den angegebenen Feature-Namen gelegt. "]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": ["featureNames = np.loadtxt(\"data/features.txt\",dtype=str)\n", "X = pd.DataFrame(data = np.vstack((X_train,X_test)),columns=featureNames)\n", "y = pd.Series(data = np.concatenate((y_train,y_test)),name='label')"]}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": ["# Checking dimensions\n", "assert(X.shape == (7352+2947, 561))\n", "assert(y.shape == (7352+2947, ))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Verwenden Sie die Methode <code>.describe()</code> von Pandas, um eine Zusammenfassung der numerischen Werte zu erhalten. Was k\u00f6nnen wir aus dieser Zusammenfassung ersehen? Schauen Sie sich insbesondere die Skalierung oder Zentrierung an.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Optionales Thema - Datenvisualisierung und -reduktion mit PCA\n", "\n", "Wie wir gesehen haben, enth\u00e4lt der Datensatz mehr als 500 Spalten (Features). Es ist nicht m\u00f6glich, alle Daten darzustellen. Wir werden nichts sehen oder erkennen k\u00f6nnen, wenn wir versuchen, eine Verteilung oder Histogramme aller Features darzustellen. Au\u00dferdem wissen wir auch nicht, welche Spalten interessant sein k\u00f6nnten. Aber was f\u00fcr ein Gl\u00fcck, dass wir eine Methode kennengelernt haben, um Datens\u00e4tze mit einer so hohen Dimension zu visualisieren: PCA!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Welche Art von Skalierung m\u00fcssen wir durchf\u00fchren, um die Anforderungen der PCA zu erf\u00fcllen? F\u00fchren Sie den erforderlichen Schritt aus.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Kopieren Sie nun die Funktion zum Plotten der kumulierten Varianz der Principal Components, die Sie zu Beginn der Aufgabe bearbeitet haben. \u00c4ndern Sie die Plotting-Funktion so, dass nur 100 Features angezeigt werden.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b>  Verwenden Sie die sklearn-PCA, um die Transformation mit allen Features von X_skaliert zu berechnen. Zun\u00e4chst geben wir keine Anzahl von Komponenten an, die wir beibehalten wollen, oder ein Verh\u00e4ltnis der erkl\u00e4rten Varianz. Plotten Sie die Varianz mit Ihrer Funktion von oben.\n", "\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b>  Reduzieren Sie nun die Anzahl der Dimensionen mittels PCA, so dass so viele Komponenten \u00fcbrig bleiben, wie n\u00f6tig sind, um mindestens 90 % der Varianz der Daten zu erkl\u00e4ren. Verwenden Sie den skalierten Datensatz und setzen Sie einen festen <code>random_state</code> (verwenden Sie die Konstante RANDOM_STATE).\n", "\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Wie viele Principal Components sind mindestens erforderlich, um 90 % der Varianz der Daten zu erfassen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Wie viel Prozent der Varianz wird von der ersten Principal Component abgedeckt? Runden Sie auf das n\u00e4chste Prozent.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Visualisieren Sie die Daten in Projektion auf die ersten beiden Principal Components.\n", "\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE \n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Beschreiben Sie die Ergebnisse des Plots. Was kann man aus den Ergebnissen schlie\u00dfen? Was ist der Nutzen der PCA?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Optionales Thema - Clustering"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> F\u00fchren Sie Clustering mit <code>KMeans</code> durch und trainieren Sie das Modell auf Daten mit reduzierter Dimensionalit\u00e4t (durch PCA).\n", "\n", "</div>\n", "\n", "Als ersten Versuch werden wir nach der Anzahl der Clusters suchen, die wir mit blo\u00dfem Auge sehen. Im allgemeinen Fall werden wir nicht wissen, nach wie vielen Clustern wir suchen sollen.\n", "\n", "Parameter des K-Means-Algorithmus:\n", "\n", "- `n_clusters` = wie oben angegeben\n", "- `n_init` = 100\n", "- `random_state` = RANDOM_STATE (f\u00fcr die Reproduzierbarkeit des Ergebnisses)\n", "\n", "Andere Parameter sollten Standardwerte haben."]}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Visualisieren Sie Ihre Ergebnisse, indem Sie die vorhergesagten Clusterbeschriftungen als Farbe f\u00fcr die Darstellung verwenden.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Wie interpretieren Sie das Ergebnis des Clustering? Beschreiben Sie den Plot.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Variieren der Anzahl von Clustern\n", "\n", "Unsere erste Sch\u00e4tzung der Anzahl von Clustern f\u00fchrte zu einem sch\u00f6nen Ergebnis. Was aber, wenn wir mehr Informationen aus dem Datensatz herausholen k\u00f6nnten, wenn wir die Anzahl der Cluster variieren?\n", "\n", "#### Ellenbogen-Methode"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Wir wollen die zuvor gelernte Ellbogenmethode verwenden, um die Anzahl der Cluster zu sch\u00e4tzen. Geben Sie daher den folgenden Code ein.\n", "</div>"]}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [], "source": ["def calculate_k_loss(X, k_min, k_max):\n", "    ### STUDENT CODE HERE\n", "\n", "    ### STUDENT CODE until HERE\n", "    \n", "    return k_values, losses"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Verwenden Sie den folgenden Code, um Ihre Funktion aufzurufen und den Verlust in Abh\u00e4ngigkeit von den k-Werten darzustellen. Verwenden Sie einen Bereich von eins bis zehn f\u00fcr den Wert von k.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 58, "metadata": {}, "outputs": [], "source": ["# call your function\n", "### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE\n", "# Plot k_values vs inertias\n", "plt.plot(k_values, losses, '-o')\n", "plt.xlabel('number of clusters k')\n", "plt.ylabel('$ J(C)$')\n", "plt.xticks(k_values)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Wie viele Cluster k\u00f6nnen wir nach der Ellbogenmethode ausw\u00e4hlen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Silhouette Ergebnis\n", "\n", "Benutzen wir die Silhouettenwertung, um einen Wert f\u00fcr $k$ zu erhalten."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> F\u00fcllen Sie den unten stehenden Code mit der Funktion aus, die wir bereits in Aufgabe 2 verwendet haben. Ermitteln Sie den Score f\u00fcr Werte von $k$ zwischen $2$ und $10$.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import silhouette_score\n", "\n", "def calculate_k_silhouette(X, k_min, k_max):\n", "    ### STUDENT CODE HERE\n", "\n", "    ### STUDENT CODE until HERE\n", "    \n", "    return ks, silhouettes\n"]}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE\n", "# Plot ks vs inertias\n", "plt.plot(ks, silhouettes, '-o')\n", "plt.xlabel('number of clusters, k')\n", "plt.ylabel('$ Silhouette score $')\n", "plt.xticks(ks)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Was ist Ihre Schlussfolgerung, wenn Sie die Ergebnisse der beiden Metriken betrachten? Welchen Wert w\u00fcrden Sie f\u00fcr $k$ w\u00e4hlen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Analysieren der gegebenen Label Informationen\n", "\n", "#### Klassenverteilung\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Lassen Sie uns herausfinden, wie viele Klassen im Datensatz tats\u00e4chlich gegeben sind. Berechnen Sie die Anzahl der eindeutigen Werte der Beschriftungen der Zielklasse. Wie viele Datenpunkte haben wir in jeder Klasse? Plotten Sie ein Histogramm f\u00fcr die Verteilung der Klassenlabels.\n", "</div>"]}, {"cell_type": "code", "execution_count": 61, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "code", "execution_count": 62, "metadata": {}, "outputs": [], "source": ["#plotting the histogram\n", "### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> [Schauen Sie sich die Datensatzbeschreibung an.](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.names) \n", "\n", "Was stellen die Labels der Klassen dar?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", "", "", "", "", "", "", "", "", "", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Anzeigen der Klassenverteilung mit PCA-Daten\n", "\n", "Um einen besseres Verst\u00e4ndnis von den tats\u00e4chlichen Klassen im Datensatz zu erhalten, wollen wir die Beschriftungen der Datenpunkte in unserem PCA-transformierten Datensatz zeigen."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Erstellen Sie ein Streudiagramm der PCA-transformierten Daten und markieren Sie die Klassen in verschiedenen Farben.\n", "</div>"]}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Wenn alles richtig funktioniert hat, sehen Sie den gleichen Plot wie zuvor in Teilaufgabe 2, aber mit Label Informationen zu jedem Datenpunkt. Welches Problem haben wir bei der Betrachtung der verschiedenen Klassen, die wir in unserem Datensatz haben? Welche Arten von Aktivit\u00e4ten sind in welchem visuell getrennten Cluster enthalten?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", "", "", "", "", "", "", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Clustering von PCA-Daten\n", "\n", "Schauen wir uns an, ob der K-Means-Algorithmus in der Lage ist, die echten sechs Cluster im Datensatz zu finden."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> F\u00fchren Sie das Clustering mit der Methode <code>KMeans</code> durch und trainieren Sie das Modell auf Daten mit reduzierter Dimensionalit\u00e4t (durch PCA). In diesem Fall geben wir einen Hinweis, nach genau 6 Clustern zu suchen, aber im allgemeinen Fall wissen wir nicht, nach wie vielen Clustern wir suchen sollten.\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optionen:\n", "\n", "- `n_clusters` = n_classes (Anzahl der eindeutigen Labels der Zielklasse)\n", "- `n_init` = 100\n", "- `random_state` = RANDOM_STATE (f\u00fcr die Reproduzierbarkeit des Ergebnisses)\n", "\n", "Andere Parameter sollten Standardwerte haben."]}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Visualisieren Sie die Daten in Projektion auf die ersten beiden Principal Components. F\u00e4rben Sie die Punkte entsprechend den erhaltenen Clustern.\n", "</div>"]}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Beschreiben Sie das Clustering-Ergebnis. Was k\u00f6nnen Sie aus dem resultierenden Diagramm erkennen?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["###  Evaluieren der Clustering-Ergebnisse anhand von Labels\n", "Die Labels der Daten k\u00f6nnen zur Evaluierung unserer Clustering-Ergebnisse verwendet werden. Es ist jedoch nicht m\u00f6glich, die gleichen Metriken zu berechnen, wie sie bei Klassifizierungsaufgaben verwendet werden. Bei der Klassifizierung ist die Ausgabe eines Klassifizierungsalgorithmus genau eine der gelernten Klassen. Im Vergleich dazu f\u00fchrt ein Clustering-Algorithmus nur eine Zuordnung zu einem Cluster durch. Aber wir wissen nicht, welche Klasse ein Cluster gefunden hat. . \n", "\n", "#### Kreuztabellierung (engl. Cross Tabulation)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Betrachten Sie die \u00dcbereinstimmung zwischen den Clustermarken und den urspr\u00fcnglichen Labels der Klassen. Berechnen Sie deshalb eine Kreuztabellierung (engl. cross tabulation) der Clusterlabels und der echten Labels der Datenpunkte.\n", "    \n", "<b>Hinweis:</b> Benutzen Sie crosstab aus der pandas-Bibliothek\n", "</div>"]}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b>  Mit welcher Art von Aktivit\u00e4ten wird der <code>KMeans</code>-Algorithmus verwechselt?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Entwickeln einer einfachen Trennungsmetrik\n", "Wir sehen, dass f\u00fcr jede Klasse (d. h. jede Aktivit\u00e4t) die Daten verschiedenen Clustern zugeordnet sind. Schauen wir uns den maximalen Prozentsatz der Objekte in einer Klasse an, die einem einzigen Cluster zugeordnet sind. Dies wird eine einfache Metrik sein, die charakterisiert, wie leicht die Klasse beim Clustern von anderen getrennt wird.\n", "\n", "Beispiel: Wenn f\u00fcr die Klasse \"walking downstairs\" (mit 1406 dazugeh\u00f6rigen Instanzen) die Verteilung der Cluster ist:\n", " - Cluster 1 - 900\n", " - Cluster 3 - 500\n", " - Cluster 6 - 6,\n", " \n", "dann wird ein solcher Anteil 900/1406 $ \\approx $ 0,64 sein.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe:</b> Berechnen Sie diesen Wert f\u00fcr alle Klassen.\n", "</div>"]}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [], "source": ["### STUDENT CODE HERE\n", "\n", "### STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Welche Aktivit\u00e4ten werden anhand der oben beschriebenen einfachen Metrik besser vom Rest getrennt als andere?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage:</b> Wie w\u00fcrden Sie die Ergebnisse im Allgemeinen bewerten? Haben Sie eine Erkl\u00e4rung f\u00fcr die Ergebnisse?\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Anhang A: Berechnung der PCA \u00fcber SVD\n", "Die Berechnung der PCA \u00fcber die Kovarianzmatrix ist fehleranf\u00e4llig und ben\u00f6tigt viel Speicherplatz. Gl\u00fccklicherweise gibt es eine Standardtechnik zur Matrixfaktorisierung, die Singul\u00e4rwertzerlegung (SVD), mit der Sie die Matrix $X$ mit den Trainingsdaten in das Skalarprodukt der drei Matrizen $U \\cdot S \\cdot V^T$ zerlegen k\u00f6nnen, wobei $V$ alle gesuchten Principal Components enth\u00e4lt, wie in der folgenden Formel dargestellt:\n", "\n", "$$\\Large V = \\begin{pmatrix}\n", "\\mid & \\mid & & \\mid \\\\\n", "{\\bf c}_{1} & {\\bf c}_{2} & \\cdots & {\\bf c}_{n}\\\\\n", "\\mid & \\mid & & \\mid \\\\\n", "\\end{pmatrix}$$\n", "Intern verwendet scikit-learn die SVD-Technik, um die PCA zu berechnen.\n", "$ \\mathbf{S}$ enth\u00e4lt die Singul\u00e4rwerte\n", "\n", "$$ \\Large \\mathbf{S} = \\small \\begin{pmatrix}\n", "\\sigma_1 &          &          &        & \\vdots &        \\\\\n", "         & \\ddots   &          & \\cdots & 0      & \\cdots \\\\\n", "         &          & \\sigma_r &        & \\vdots &        \\\\\n", "\\hline\n", "         &  \\vdots  &          &        & \\vdots &        \\\\\n", "\\cdots   &  0       & \\cdots   & \\cdots & 0      & \\cdots \\\\\n", "         &  \\vdots  &          &        & \\vdots &        \\\\\n", "\\end{pmatrix} $$\n", "\n", "Nun wissen wir, dass die Vektoren $c_1$ bis $c_n$ die Principal Components sind, die in die Richtung der maximalen Varianz zeigen. Aber was bedeuten die Singul\u00e4rwerte in $\\mathbf{S}$? Tats\u00e4chlich entsprechen die quadrierten Singul\u00e4rwerte den Eigenwerten von $X^T\\cdot X$ (Eigenwert $\\neq$ Singul\u00e4rwert!). Der Eigenwert ist proportional zur erkl\u00e4rten Varianz in Richtung des zugeh\u00f6rigen Eigenvektors. Folglich sind auch die quadrierten Singul\u00e4rwerte proportional zur Varianz in Richtung der Principal Components (neue Merkmalsdimensionen). Die Summe aller quadrierten Singul\u00e4rwerte ist gleich der Summe der quadrierten Abst\u00e4nde der Punkte von ihrem mehrdimensionalen Mittelwert. Da die Kovarianzmatrix auch in Form von $X^T\\cdot X$ geschrieben werden kann, ist die Proportionalit\u00e4t der quadrierten Singul\u00e4rwerte zur erkl\u00e4rten Varianz gleich den Eigenwerten der Kovarianzmatrix.\n", "$$\\Large \\frac{\\sigma_i^2}{\\Sigma_i \\sigma_i^2} = \\frac{\\lambda_i}{\\Sigma_i \\lambda_i}$$\n", "mit $\\lambda$ als Eigenwert von $cov(X)$. Jeder quadrierte Singul\u00e4rwert erkl\u00e4rt also den gleichen Teil der Varianz wie die Eigenwerte der urspr\u00fcnglichen Kovarianzmatrix.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Anhang B: K-Means \n", "\n", "### Bekannte Probleme\n", "\n", "K-Means hat eine Zeitkomplexit\u00e4t von $O(n^{d k+1})$. Das bedeutet, dass die Zeit f\u00fcr die Durchf\u00fchrung des Clustering exponentiell mit der Anzahl der Cluster k und der Anzahl der Dimensionen d und polynomiell mit der Anzahl der Beobachtungen n ansteigt. \n", "Es gibt jedoch einige Heuristiken, um damit umzugehen; ein Beispiel ist MiniBatch K-means, das Teile (Batches) von Daten nimmt, anstatt den gesamten Datensatz anzupassen, und dann Zentroide verschiebt, indem es den Durchschnitt der vorherigen Schritte nimmt. Vergleichen Sie die Implementierung von K-means und MiniBatch K-means in der [sckit-learn Dokumentation](http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html).\n", "\n", "Die [Implementierung](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) des Algorithmus unter Verwendung von `scikit-learn` hat seine Vorteile, wie z. B. die M\u00f6glichkeit, die Anzahl der Initialisierungen mit dem Funktionsparameter `n_init` anzugeben, was uns erm\u00f6glicht, robustere Zentroide zu identifizieren. Au\u00dferdem k\u00f6nnen diese L\u00e4ufe parallel durchgef\u00fchrt werden, um die Rechenzeit zu verringern.\n", "\n", "#### K-means Clustering mit besserer Initialisierung: KMeans++\n", "Mit gen\u00fcgend Zeit wird K-means immer konvergieren, auch wenn dies zu einem lokalen Minimum f\u00fchrt. Wie wir gesehen haben, ist dies stark von der Initialisierung der Zentroide abh\u00e4ngig. Dies f\u00fchrte zur Entwicklung vieler Ans\u00e4tze, um einer m\u00f6glichen schlechten Clustering zu begegnen: Z. B. wird die Berechnung oft mehrmals durchgef\u00fchrt, mit verschiedenen zuf\u00e4lligen Initialisierungen der Zentroide. Eine andere M\u00f6glichkeit w\u00e4re, das Clustering mit dem geringsten Verlust zu nehmen. Die gebr\u00e4uchlichste Heuristik ist jedoch k-means++: Bei der Initialisierung der Clusterzentren w\u00e4hlen wir $\\mu_{i}$ nicht zuf\u00e4llig aus allen Clustern aus. Stattdessen w\u00e4hlen wir $\\mu_{i}$ sequentiell aus, und zwar mit einer Wahrscheinlichkeit, die proportional zum minimalen quadratischen Abstand zu allen anderen Zentroiden ist. Dadurch werden die Zentroide so initialisiert, dass sie (im Allgemeinen) weit voneinander entfernt sind, was zu nachweislich besseren Ergebnissen f\u00fchrt als eine zuf\u00e4llige Initialisierung. Nachdem diese Zentren initialisiert sind, f\u00fchren wir k-means wie gewohnt aus.\n", "\n", "#### Pseudocode f\u00fcr K-means++\n", "\n", "Given: Dataset $\\large \\mathbf{X} \\in \\mathbb{R}^{m \\times d}$, number of clusters $\\large k$\n", "\n", "- Initialize:\n", "\n", "  - Choose $\\large \\mu_{1}$ = Random$\\large (\\mathbf{X})$\n", "\n", "- For $\\large j = 2, \\dots,k$:\n", "\n", "  - For every point $\\large x_i$ in $\\mathbf{X}$:\n", "\n", "    - Get the nearest cluster $\\large \\mu_{j'}$  ($\\large j' < j$) center already found\n", "    \n", "    - Compute the distance $\\large D(x_i)$ to the nearest cluster center $\\large \\mu_{j'}$\n", "    \n", "  - Compute the probabilty $\\large p_{x_i} \\propto D(x_i)^2$ of every point $\\large x_i$ to be chosen as new center\n", "    \n", "  - Choose $\\large \\mu^{j}$ =  Random $\\large ( \\mathbf{X}, p )$\n", " \n", "### Ein Vergleich der Clustering-Algorithmen in scikit-learn\n", "Entnommen aus https://scikit-learn.org/stable/modules/clustering.html#clustering\n", "\n", "| Methodenname                  | Parameter                                              | Skalierbarkeit                                                 | Anwendungsfall                                                                   | Geometrie (verwendete Metrik)                       |\n", "|------------------------------|---------------------------------------------------------|-------------------------------------------------------------|---------------------------------------------------------------------------|----------------------------------------------|\n", "| K-Means | Anzahl der Cluster | Sehr gro\u00dfe n_Stichproben, mittlere n_Cluster mit MiniBatch-Code | Allgemeing\u00fcltig, gleichm\u00e4\u00dfige Clustergr\u00f6\u00dfe, flache Geometrie, nicht zu viele Cluster | Abst\u00e4nde zwischen Punkten |\n", "| Affinity Propagation | D\u00e4mpfung, Datenpr\u00e4ferenz | Nicht skalierbar mit n_Stichproben | Viele Cluster, ungleichm\u00e4\u00dfige Clustergr\u00f6\u00dfe, nicht flache Geometrie | Graphenabstand (z. B. Nearest-Neighbor-Graph) |\n", "| Mean-Shift | Bandbreite | Nicht skalierbar mit n_Samples | Viele Cluster, ungleiche Clustergr\u00f6\u00dfe, nicht flache Geometrie | Abst\u00e4nde zwischen Punkten |\n", "| Spektrales Clustering | Anzahl der Cluster | Mittlere n_Stichproben, kleine n_Cluster | Wenige Cluster, gleichm\u00e4\u00dfige Clustergr\u00f6\u00dfe, nicht flache Geometrie | Graphenabstand (z. B. Nearest-Neighbor-Graph) |\n", "| Ward hierarchisches Clustering | Anzahl der Cluster | Gro\u00dfe n_Stichproben und n_Cluster | Viele Cluster, m\u00f6glicherweise Konnektivit\u00e4tsbeschr\u00e4nkungen | Abst\u00e4nde zwischen Punkten |\n", "| Agglomeratives Clustering | Anzahl der Cluster, Verkn\u00fcpfungstyp, Abstand | Gro\u00dfe n_Stichproben und n_Cluster | Viele Cluster, evtl. Konnektivit\u00e4tsbeschr\u00e4nkungen, nicht euklidische Abst\u00e4nde | Beliebiger paarweiser Abstand |\n", "| DBSCAN | Nachbarschaftsgr\u00f6\u00dfe | Sehr gro\u00dfe n_Stichproben, mittlere n_Cluster | Nicht flache Geometrie, ungleiche Clustergr\u00f6\u00dfen | Abst\u00e4nde zwischen n\u00e4chstgelegenen Punkten |\n", "| Gau\u00dfsche Mischungen | viele | Nicht skalierbar | Flache Geometrie, gut f\u00fcr Dichtesch\u00e4tzung | Mahalanobis-Distanzen zu Zentren |\n", "| Birch | Verzweigungsfaktor, Schwellenwert, optionaler globaler Clusterer. | Gro\u00dfe n_Cluster und n_Stichproben | Gro\u00dfer Datensatz, Ausrei\u00dferentfernung, Datenreduktion.                           | Euklidischer Abstand zwischen Punkten |\n", "\n", "### Weitere Metriken zur Bewertung von Clustering-Ergebnissen\n", "Alle nachfolgend beschriebenen Metriken sind in `sklearn.metrics` implementiert.\n", "\n", "**Adjusted Rand Index (ARI)**\n", "\n", "Hier wird davon ausgegangen, dass die wahren Labels der Objekte bekannt sind. Diese Metrik h\u00e4ngt nicht von den Werten der Labels ab, sondern von der Aufteilung der Datencluster. Sei $N$ die Anzahl der Beobachtungen in einem Datensatz. Sei $a$ die Anzahl der Beobachtungspaare mit den gleichen Labels, die sich im gleichen Cluster befinden, und sei $b$ die Anzahl der Beobachtungen mit unterschiedlichen Labels, die sich in verschiedenen Clustern befinden. Der Rand-Index kann mit der folgenden Formel berechnet werden: $$\\text{RI} = \\frac{2(a + b)}{n(n-1)}.$$ \n", "Mit anderen Worten, er bewertet den Anteil der Beobachtungen, f\u00fcr die diese Aufteilungen (Ausgangs- und Clustering-Ergebnis) konsistent sind. Der Rand-Index (RI) bewertet die \u00c4hnlichkeit der beiden Splits der gleichen Datenpunkte. Damit dieser Index f\u00fcr jedes Clustering-Ergebnis mit beliebigem $n$ und beliebiger Anzahl von Clustern nahe Null ist, muss er skaliert werden, daher der Adjusted Rand Index: $$\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}.$$\n", "\n", "Diese Metrik ist symmetrisch und h\u00e4ngt nicht von der Label-Permutation ab. Daher ist dieser Index ein Ma\u00df f\u00fcr den Abstand zwischen verschiedenen Datensplits. $\\text{ARI}$ nimmt Werte im Bereich $[-1, 1]$ an. Negative Werte zeigen die Unabh\u00e4ngigkeit von Splits an, und positive Werte zeigen an, dass diese Splits konsistent sind (sie stimmen mit $\\text{ARI} = 1$ \u00fcberein).\n", "\n", "**Adjusted Mutual Information (AMI)**\n", "\n", "Diese Metrik ist \u00e4hnlich wie $\\text{ARI}$. Sie ist ebenfalls symmetrisch und h\u00e4ngt nicht von den Werten und der Permutation der Labels ab. Sie ist durch die Funktion [Entropie](https://en.wikipedia.org/wiki/Entropy_(information_theory) definiert und interpretiert eine Datenaufteilung als diskrete Verteilung (die Wahrscheinlichkeit der Zuordnung zu einem Cluster ist gleich dem Prozentsatz der Objekte in diesem Cluster). Der $MI$-Index ist definiert als die [mutual information](https://en.wikipedia.org/wiki/Mutual_information) f\u00fcr zwei Verteilungen, die der Aufteilung der Datenpunkte in Cluster entsprechen. Intuitiv misst die gegenseitige Information den Anteil der Information, den beide Clusteraufteilungen gemeinsam haben, d. h. wie die Information \u00fcber eine von ihnen die Unsicherheit der anderen verringert.\n", "\n", "\u00c4hnlich wie der $\\text{ARI}$ ist auch der $\\text{AMI}$ definiert. Dies erlaubt uns, den Anstieg des $MI$-Index mit der Anzahl der Cluster loszuwerden. Der $\\text{AMI}$ liegt in dem Bereich $[0, 1]$. Werte nahe Null bedeuten, dass die Splits unabh\u00e4ngig sind, und solche nahe 1 bedeuten, dass sie \u00e4hnlich sind (mit vollst\u00e4ndiger \u00dcbereinstimmung bei $\\text{AMI} = 1$).\n", "\n", "**Homogenit\u00e4t, Vollst\u00e4ndigkeit,V-measure**\n", "\n", "Formal werden diese Metriken auch auf der Grundlage der Entropiefunktion und der bedingten Entropiefunktion definiert, wobei die Datenaufteilungen als diskrete Verteilungen interpretiert werden: $$h = 1 - \\frac{H(C\\mid K)}{H(C)}, c = 1 - \\frac{H(K\\mid C)}{H(K)},$$\n", "wobei $K$ ein Clustering-Ergebnis ist und $C$ die anf\u00e4ngliche Aufteilung ist. Daher bewertet $h$, ob jeder Cluster aus Objekten der gleichen Klasse besteht, und $c$ misst, wie gut die Objekte der gleichen Klasse zu den Clustern passen. Diese Metriken sind nicht symmetrisch. Beide liegen im Bereich $[0, 1]$, und Werte, die n\u00e4her bei 1 liegen, zeigen genauere Clustering-Ergebnisse an. Die Werte dieser Metriken sind nicht skaliert wie die $\\text{ARI}$- oder $\\text{AMI}$-Metriken und h\u00e4ngen daher von der Anzahl der Cluster ab. Ein zuf\u00e4lliges Clustering-Ergebnis wird keine Metrikwerte nahe Null haben, wenn die Anzahl der Cluster gro\u00df genug ist und die Anzahl der Objekte klein ist. In einem solchen Fall w\u00e4re es sinnvoller, $\\text{ARI}$ zu verwenden. Bei einer gro\u00dfen Anzahl von Beobachtungen (mehr als 100) und einer Anzahl von Clustern von weniger als 10 ist dieses Problem jedoch weniger kritisch und kann vernachl\u00e4ssigt werden.\n", "\n", "Das $V$-Ma\u00df ist eine Kombination aus $h$, und $c$ und ist deren harmonisches Mittel:\n", "$$v = 2\\frac{hc}{h+c}.$$\n", "Es ist symmetrisch und misst, wie konsistent zwei Clustering-Ergebnisse sind."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 4}