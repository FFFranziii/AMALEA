{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Sch\u00f6ne Nachbarschaft"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Importe"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## K-N\u00e4chste Nachbarn (KNN) (engl. K-Nearest-Neighbors)\n", "\n", "### Was sind KNN ?\n", "\n", "Lassen Sie uns zun\u00e4chst ein paar Definitionen und Notationen festhalten. Im Folgenden werden wir $x$ zur Bezeichnung eines *Features* (auch Pr\u00e4diktor, Attribut) und $y$ zur Bezeichnung des *Targets* (auch Label, Klasse) verwenden, welches wir vorhersagen wollen.\n", "\n", "KNN geh\u00f6rt zur Familie der Algorithmen f\u00fcr **\u00fcberwachtes Lernen**. Informell bedeutet dies, dass wir einen gelabelten Datensatz erhalten, der aus Trainingsbeobachtungen $(x,y)$ besteht, und wir m\u00f6chten die Beziehung zwischen $x$ und $y$ erfassen. Formaler ausgedr\u00fcckt: Unser Ziel ist es, eine Funktion $h:X\u2192Y$ zu lernen, so dass bei einer Beobachtung $x, h(x)$ die entsprechende Ausgabe $y$ sicher vorhersagen kann.\n", "\n", "### Einf\u00fchrung\n", "\n", "In der Klassifizierungsumgebung des K-N\u00e4chsten-Nachbarn-Algorithmus wird eine Mehrheitsabstimmung zwischen den K-\u00e4hnlichsten Instanzen zu einer gegebenen \"ungesehenen\" Beobachtung gebildet. Die \u00c4hnlichkeit l\u00e4sst sich anhand einer Abstandsmetrik zwischen zwei Datenpunkten definieren. Die als $x = <a_1(x),a_2(x),...,a_n(x)>$ gegebenen Instanzen werden als Punkte im n-dimensionalen Raum $\\mathbb{R}^n$ dargestellt. Ihre Beziehungen/Distanzen k\u00f6nnen folgenderma\u00dfen formuliert werden:\n", "\n", "\\begin{equation}\n", "d(x_i,x_j) \\equiv \\sqrt{\\sum_{r=0}^{n-1} (a_r(x_i) - a_r(x_j))^2}\n", "\\end{equation}\n", "\n", "In diesem Fall wird als Abstandsmetrik der euklidische Abstand gew\u00e4hlt, welcher eine g\u00e4ngige Wahl ist. Andere Abstandsmetriken k\u00f6nnen f\u00fcr eine bestimmte Umgebung besser geeignet sein, z. B. der Manhatten-, Tschebyscheff- und Hamming-Abstand. \n", "\n", "Eine Funktion $h$ wird dann aus $\\mathbb{R}^n$ $\\rightarrow$ $V$ gelernt, wobei $V$ eine endliche Menge aller m\u00f6glichen Klassen darstellt. \n", "\n", "#### Algorithmus\n", "\n", "Jede gegebene Instanz $x_i$ wird zur Liste der Trainingsbeispiele hinzugef\u00fcgt.  \n", "\n", "**Schlussfolgerung:**\n", "\n", "Eine neue Instanz $x_q$ muss klassifiziert werden und $x_1,...,x_k$ sind die $k$ n\u00e4chstgelegenen Instanzen zu $x_q$ (diese sind nach Berechnung aller Abst\u00e4nde zwischen Trainingsbeispielen und der neuen Instanz bekannt). Beachten Sie, dass $K$ normalerweise ungerade ist, um Gleichstandssituationen zu vermeiden.\n", "Folglich ergibt sich die neue Klassifikation von $x_q$ durch:\n", "\n", "\\begin{equation}\n", "h(x_q) \\leftarrow arg\\max_{v \\in V}\\sum_{i=1}^{k}\\delta (v,y_i)\n", "\\end{equation}\n", "\n", "mit \n", "\\begin{equation}\n", "\\delta (a,b) = \\begin{cases}\n", "1, \\text{if } a=b \\\\\n", "0, else\n", "\\end{cases}\n", "\\end{equation}\n", " \n", "#### Beispiel: \n", "\n", "Klassifikation mit $K=5$:\n", "\n", "<img src=\"images/knn_5.png\" width=\"800\">\n", "\n", "__$\\rightarrow$Die neue Instanz wird als Quadrat klassifiziert.__\n", "\n", "Klassifikation mit $K=1$:\n", "<img src=\"images/knn_1.png\" width=\"800\">\n", "\n", "__$\\rightarrow$Die neue Instanz wird als Dreieck klassifiziert.__ \n", "\n", "\n", "\n", "\n", "Wenn $k = 1$ ist, sind die Entscheidungsgrenzen letztendlich die gleichen wie in einem [Voronoi](https://en.wikipedia.org/wiki/Voronoi_diagram) Diagramm.  W\u00e4hrend bei Kreisen der Blickpunkt darin besteht, die n\u00e4chsten K n\u00e4chstgelegenen Punkte zur neuen Instanz zu finden, zeigt ein Voronoi-Diagramm Regionen, in denen neue Instanzen den bekannten Datenpunkten zugeordnet werden. (Der Blickpunkt liegt nun auf den Quadraten und Dreiecken, die ihrer Klasse neue Punkte neben sich zuordnen).\n", "\n", "**Zus\u00e4tzliche Information:**\n", "\n", "- Im Allgemeinen ist es oft sinnvoll, die Input Vektoren zu normalisieren, damit die Dimensionen des Inputs nicht so stark verzerrt werden.\n", "\n", "- Es besteht die M\u00f6glichkeit, abstandsbasierte Gewichte f\u00fcr Instanzen zu verwenden, anstatt einheitlich bekannte Instanzen zu ber\u00fccksichtigen. Daher \u00e4ndert sich die oben angegebene Gleichung in:\n", "\n", "    \\begin{equation}\n", "    f(x_q) \\leftarrow arg\\max_{v \\in V}\\sum_{i=1}^{k} w_i \\delta (v,c(x_i))\n", "    \\end{equation}\n", "\n", "    und die Gewichte sind gegeben durch:\n", "\n", "    \\begin{equation}\n", "     w_i \\equiv \\frac{1}{d(x_q,x_i)^2}\n", "    \\end{equation}\n", "    \n", "- __Cover und Hart 1967__: Mit $n \\rightarrow \\infty$, ist der 1-NN-Fehler nicht mehr als doppelt so gro\u00df wie der Fehler des Bayes Optimal Klassifikators.(Analog f\u00fcr k>1.) [Quelle](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)\n", "\n", "- __Fluch der Dimensionalit\u00e4t__: Der induktive Bias des K-NN-Algorithmus besteht darin, dass \u00e4hnliche Punkte Labels teilen. In hochdimensionalen R\u00e4umen trifft diese Annahme nicht mehr so gut wie in niederdimensionalen R\u00e4umen. Dies liegt daran, dass Punkte nicht nahe beieinander liegen, wenn sie in jeder Dimension gleichm\u00e4\u00dfig liegen. Siehe auch: [Quelle](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)\n", "\n", "- Reduzieren Sie hochdimensionale Daten durch PCA oder SVD. Anschlie\u00dfend sollten die intrinsischen Dimensionen der Daten niedriger dimensioniert sein oder diese nutzen nicht alle Dimensionen, in denen die Daten vorliegen\n", "\n", "- K-NN-Algorithmus wird langsam, wenn n oder die Dimensionen d zunehmen. Dies k\u00f6nnte zu einer nicht durchf\u00fchrbaren Inferenzzeit f\u00fchren\n", "\n", "- K-NN-Algorithmus wird mit zunehmendem n immer genauer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.1:</b> Laden Sie die vorbereiteten Training- und Testdatens\u00e4tze, da wir nur GridSearchCV verwenden werden\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["# Load the prepared datasets training and test set, because we are only going to use GridSearchCV\n", "\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.2:</b> Normalisieren Sie die Daten mit MinMaxScaling. Bitte verwenden Sie die Formel und achten Sie darauf, dass der Datentyp aufgrund der Visualisierung sp\u00e4ter in diesem Notebook noch ein pandas Dataframe ist__(Benutzen Sie nicht den MinMaxScaler!)__. Achten Sie ebenso auf identische Variablenbezeichnungen und mehrfache Zellenausf\u00fchrung.\n", "</div>"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# Normalize the data\n", "\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# Imports the necessary modules\n", "\n", "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n", "from matplotlib.colors import ListedColormap\n", "num_neighbors = 1\n", "radius = 100"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Die k-Nachbarn-Klassifikation in KNeighborsClassifier ist die am h\u00e4ufigsten verwendete Technik. Die optimale Wahl des Wertes ist stark datenabh\u00e4ngig: Im Allgemeinen unterdr\u00fcckt ein gr\u00f6\u00dferes k die Auswirkungen von Rauschen, reduziert allerdings die Deutlichkeit der Klassifizierungsgrenzen.\n", "\n", "In F\u00e4llen, in denen die Daten nicht gleichm\u00e4\u00dfig verteilt sind, kann die radiusbasierte Nachbarschaftsklassifizierung in RadiusNeighborsClassifier eine bessere Wahl sein. Hier gibt der/die NutzerIn einen festen Radius vor, so dass Punkte in sp\u00e4rlicheren Nachbarschaften eine geringere Anzahl an n\u00e4chsten Nachbarn f\u00fcr die Klassifizierung verwenden. F\u00fcr hochdimensionale Parameterr\u00e4ume wird diese Methode aufgrund des sogenannten \"Fluch der Dimensionalit\u00e4t\" weniger effektiv.\n", "\n", "Die grundlegende Klassifikation der n\u00e4chsten Nachbarn verwendet einheitliche Gewichtungen: Das hei\u00dft, der einem Abfragepunkt zugewiesene Wert wird aus einer einfachen Mehrheitsentscheidung der n\u00e4chsten Nachbarn berechnet. Unter Umst\u00e4nden ist es besser, die Nachbarn so zu gewichten, dass n\u00e4here Nachbarn mehr zur Anpassung beitragen. Dies kann durch das Schl\u00fcsselwort weights erreicht werden. Der Standardwert weights = 'uniform' weist jedem Nachbarn einheitliche Gewichte zu. weights = 'distance' weist Gewichtungen proportional zum Kehrwert der Entfernung vom Abfragepunkt zu. Alternativ kann eine benutzerdefinierte Funktion des Abstands geliefert werden, um die Gewichte zu berechnen.\" [Quelle](https://scikit-learn.org/stable/modules/neighbors.html)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["neigh = KNeighborsClassifier(n_neighbors=num_neighbors)\n", "neigh.fit(x_train, y_train)\n", "\n", "neigh_r = RadiusNeighborsClassifier(radius)\n", "neigh_r.fit(x_train, y_train)\n", "\n", "print(\"Test score of K-Nearest Neighbor: %f\" %neigh.score(x_test, y_test))\n", "print(\"Test score of K-Nearest Neighbor with radius: %f\" %neigh_r.score(x_test, y_test))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Optimierung"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Optimale Anzahl an Nachbarn f\u00fcr den konventionellen Klassifikator (KNN) (engl. Conventional Classifier)\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.3:</b> \n", "<ul>\n", "<li> Optimieren Sie nicht nur in Bezug auf einen, sondern auf zwei Parameter\n", "<li> Optimieren Sie k in nearest_n_arr sowie die Gewichtsverteilung.\n", "<li> Nutzen Sie GridSearchCV und einen Dictionary f\u00fcr k_params (in der KNN-Aufgabe m\u00fcssen Sie nicht StratifiedKfold verwenden, sondern nur den Parameter cv, der eine stratifizierte Faltung durchf\u00fchrt, aber ohne den Parameter random_state.)\n", "\n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Nearest Neighbors values to optimize the model\n", "nearest_n_arr = range(1,20)  # natural numbers\n", "\n", "# Necessary for plotting\n", "length_a = len(nearest_n_arr)  \n", "\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["neighourhood = k_params['n_neighbors']\n", "scores = k_model.cv_results_['mean_test_score']\n", "scores_std = k_model.cv_results_['std_test_score']\n", "\n", "# Resorting for plots\n", "scores = np.append(scores[0::2], scores[1::2])\n", "scores_std = np.append(scores_std[0::2], scores_std[1::2])\n", "\n", "neighourhood = k_params['n_neighbors']\n", "\n", "plt.figure().set_size_inches(20, 5)\n", "plt.xlabel('Number of Neighbors')\n", "plt.xlim(neighourhood[0],neighourhood[-1])\n", "plt.ylabel('Mean Test Score')\n", "plt.plot(neighourhood, scores[:length_a],'g--',\n", "        label='uniform')\n", "plt.plot(neighourhood, scores[length_a:]\n", "         , 'b--',label='distance')\n", "plt.title('Optimal Number of Neighbors w.r.t. weight distribution')\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Das beste Modell erhalten\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.4:</b> \n", "<ul>\n", "<li> Ermitteln Sie die besten Parameter und die zugeh\u00f6rigen Ergebnisse.\n", "<li> Geben Sie diese aus! \n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# Get the params, and the test score of the best model, print the values\n", "\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Optimaler Radius f\u00fcr den Radius Klassifikator (engl. RadiusClassifier)\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.5:</b> \n", "<ul>\n", "<li> Analog wie zuvor: Benennen Sie Ihren GridSearchCV k_r_model und verwenden Sie k_r_params (Sehen Sie sich den n\u00e4chsten Code-Block an, um Konsistenz zu gew\u00e4hrleisten)\n", "<li> Optimieren Sie nun den Dictionary mit Radius und Gewichten\n", "<li> Sie k\u00f6nnen nun entscheiden, ob Sie Dezimalschritte oder nat\u00fcrliche Zahlen f\u00fcr die Radien w\u00fcnschen.\n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# Choose for different radii\n", "radius_arr = np.arange(0.6, 1.2, 0.1)  # decimal steps\n", "radius_arr = range(1,10)  # natural numbers\n", "\n", "# Necessary for plotting\n", "length_r_a = len(radius_arr)  \n", "\n", "\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["neighourhood_r = k_r_params['radius']\n", "scores_r = k_r_model.cv_results_['mean_test_score']\n", "scores_std_r = k_r_model.cv_results_['std_test_score']\n", "\n", "scores_r = np.append(scores_r[0::2], scores_r[1::2])\n", "scores_std_r = np.append(scores_std_r[0::2], scores_std_r[1::2])\n", "\n", "plt.figure().set_size_inches(20, 5)\n", "plt.title('Optimal Radius w.r.t. weight distribution')\n", "plt.xlim(neighourhood_r[0],neighourhood_r[-1])\n", "plt.xlabel('Radius')\n", "plt.ylabel('Mean Test Score')\n", "plt.plot(neighourhood_r, scores_r[:length_r_a],'g--',\n", "        label='uniform')\n", "plt.plot(neighourhood_r, scores_r[length_r_a:],'b--',\n", "        label='distance')\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.6:</b> Was passiert, wenn Sie einen Radius von 0 nehmen?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.7\n", "    :</b> K\u00f6nnen Sie sich vorstellen, warum die Korrektklassifikationsrate (engl. accuracy) des Modells ab einem bestimmten Radius stagniert?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.8:</b>  \u00c4ndern Sie das Radius-Array von nur nat\u00fcrlichen Zahlen auf Dezimalzahlen wie 0.6, 0.7.,... (kommentieren Sie die Zeile mit den nat\u00fcrlichen Zahlen aus). Beachten Sie, dass f\u00fcr einige Radien keine Nachbarn gefunden werden k\u00f6nnen. Passen Sie daher die untere Grenze an, bis mindestens ein Nachbar gefunden wird.\n", "</li>\n", "</ul>\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.9:</b> Warum macht der zweite Ansatz mehr Sinn?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Das beste Modell erhalten\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.10:</b> Ermitteln Sie f\u00fcr den Radius Klassifikator das beste Modell und dessen entsprechende Parameter. Geben Sie diese mit dem Score aus.\n", "</div>"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["# Get the params, and the test score of the best model, print the values\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Vergleich"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.11:</b> Welches Modell w\u00fcrden Sie nehmen, Radius Klassifikator (engl.RadiusClassifier) oder konventioneller Klassifikator (engl. Conventional Classifier) und warum? Vergleichen Sie daher die trainierten Klassifikatoren mit der besten Parametereinstellung und die Ergebnisse von GridSearchCV.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.12:</b> Sind die Gewichte in den zwei besten Modellen einheitlich?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualisierung\n", "\n", "Wir nehmen die beiden Features des Trainingssatzes und schauen, wo die Entscheidungsgrenzen f\u00fcr ein darauf trainiertes KNN gezogen werden."]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["def plot_knn(n_neighbors:int, radius:float, f_names:list, X:np.array, y:np.ndarray):\n", "    \n", "    # Create color maps\n", "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n", "    cmap_bold = ListedColormap(['#FF0000','#0000FF'])\n", "    \n", "    h = .02\n", "    for weights in ['uniform', 'distance']:\n", "        # we create an instance of Neighbours Classifier and fit the data.\n", "        clf = KNeighborsClassifier(n_neighbors, weights=weights)\n", "        clf_r = RadiusNeighborsClassifier(radius=radius, weights= weights, outlier_label=0)\n", "        clf.fit(X, y)\n", "        clf_r.fit(X,y)\n", "        \n", "        # Plot the decision boundary. For that, we will assign a color to each\n", "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n", "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n", "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n", "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n", "                             np.arange(y_min, y_max, h))\n", "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n", "        Z2 = clf_r.predict(np.c_[xx.ravel(), yy.ravel()])\n", "        \n", "        # Put the result into a color plot\n", "        Z = Z.reshape(xx.shape)\n", "        Z2 = Z2.reshape(xx.shape)\n", "         \n", "        f,ax = plt.subplots(1,2,figsize=(20,5))\n", "        \n", "        ax[0].pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n", "        sc1 = ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n", "                    edgecolor='k', s=20)\n", "\n", "        ax[0].set_xlim(xx.min(), xx.max())\n", "        ax[0].set_ylim(yy.min(), yy.max())\n", "        ax[0].set_xlabel(f_names[0])\n", "        ax[0].set_ylabel(f_names[1])\n", "        ax[0].set_title(\"K-Nearest Neighbor with (k= %i, weights = '%s')\"\n", "                       % (n_neighbors, weights))\n", "        f.colorbar(sc1,ax = ax[0]) \n", "        \n", "        ax[1].pcolormesh(xx, yy, Z2, cmap=cmap_light, shading='auto')\n", "        sc2 = ax[1].scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n", "                    edgecolor='k', s=20)\n", "        \n", "        ax[1].set_xlim(xx.min(), xx.max())\n", "        ax[1].set_ylim(yy.min(), yy.max())\n", "        ax[1].set_xlabel(f_names[0])\n", "        ax[1].set_ylabel(f_names[1])\n", "        ax[1].set_title(\"Radius Neighbors Classifier with (R= %0.3f, weights = '%s')\"\n", "                       % (radius, weights))\n", "        f.colorbar(sc1, ax = ax[1]) \n", "       \n", "    plt.show()"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# Reconsider all features we have\n", "x_test.columns"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["# Choose features to compare\n", "features_compared = ['Pclass', 'Sex']\n", "\n", "# Plot them\n", "plot_knn(2, 0.3, features_compared, np.array(x_test[features_compared]), y_test.values)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.13:</b> Welche Funktionen wurden in der Visualisierung verglichen?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.14:</b> Warum sind in der Visualisierung nur 6 Punkte vorhanden?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwortr:</b> Da es f\u00fcr jedes Feature nur 3 bzw. 2 Werte gibt, ergeben sich 6 Kombinationen.\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.15:</b> Sind die zuvor ermittelten optimalen Werte f\u00fcr KNN und RNN (\u00fcber GridSearchCV) f\u00fcr diese Plots zuverl\u00e4ssig?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.16:</b>  Was passiert mit den Entscheidungsgrenzen, wenn Sie k erh\u00f6hen? Sie k\u00f6nnten die Features \u00e4ndern, um das zu sehen.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.17:</b> Plotten Sie stetige Features, wenn m\u00f6glich.\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 3.3.18:</b> Lassen Sie die Normalisierung gleich zu Beginn dieser Aufgabe weg.\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.19:</b>  Ist es eine gute Idee, nicht zu normalisieren?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualisierung mit verschiedenen Methoden\n", "\n", "Im Folgenden werden einige M\u00f6glichkeiten zur Visualisierung von Daten in h\u00f6heren Dimensionen verwendet, da wir mit den knn_plots bisher nur zwei Dimensionen betrachtet haben."]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "from sklearn.decomposition import PCA as sklearnPCA\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n", "from sklearn.datasets import make_blobs\n", "\n", "from pandas.plotting import parallel_coordinates"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Dimensionsreduktion mit PCA\n", "\n", "Die Dimensionsreduktion mit PCA kann auch f\u00fcr Preprocessing Zwecke verwendet werden (auch mit K-NNs). Diese wird hier aber nur f\u00fcr Visualisierungszwecke verwendet."]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["pca = sklearnPCA(n_components=2) #2-dimensional PCA\n", "transformed = pd.DataFrame(pca.fit_transform(x_train))"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,5))\n", "plt.scatter(transformed[y_train==0][0], transformed[y_train==0][1], label='Nicht \u00fcberlebt', c='red')\n", "plt.scatter(transformed[y_train==1][0], transformed[y_train==1][1], label='\u00dcberlebt', c='green')\n", "plt.title(\"PCA Plot der Titanic-Features\")\n", "\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Parallele Koordinaten\n", "\n", "Eine Zeile beschreibt einen Datenpunkt in Ihren Daten. Wenn Sie sich \"stetige\" Werte ansehen (mehr als nur eine Handvoll diskreter Werte), k\u00f6nnen Sie sehen, dass zum Beispiel lange Namen dazu tendieren, zu \u00fcberleben."]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["data_norm = pd.concat([x_train, y_train], axis=1)\n", "\n", "# Perform parallel coordinate plot\n", "plt.figure(figsize=(20,5))\n", "parallel_coordinates(data_norm, 'Survived')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Frage 3.3.20:</b>  Was passiert, wenn die Daten bei diesen Methoden nicht normalisiert werden? Pr\u00fcfen Sie selbst.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Ihre Antwort:</b></div>", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Siehe auch: http://www.apnorton.com/blog/2016/12/19/Visualizing-Multidimensional-Data-in-Python/"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Weitere Methoden\n", "\n", "\n", "- Bagging\n", "- Logistic Regression\n", "- Random Forests\n", "- Kombination aus Bagging und GridSearchCV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.2"}}, "nbformat": 4, "nbformat_minor": 4}